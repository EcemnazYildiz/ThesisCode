{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcd2bb2",
   "metadata": {},
   "source": [
    "## Loading Packages & Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a234b69d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1     ✔ purrr   0.3.2\n",
      "✔ tibble  3.1.0     ✔ dplyr   1.0.5\n",
      "✔ tidyr   1.1.3     ✔ stringr 1.4.0\n",
      "✔ readr   1.3.1     ✔ forcats 0.4.0\n",
      "Warning message:\n",
      "“package ‘tibble’ was built under R version 3.6.3”Warning message:\n",
      "“package ‘tidyr’ was built under R version 3.6.3”Warning message:\n",
      "“package ‘dplyr’ was built under R version 3.6.3”── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::between()   masks data.table::between()\n",
      "✖ dplyr::filter()    masks stats::filter()\n",
      "✖ dplyr::first()     masks data.table::first()\n",
      "✖ dplyr::lag()       masks stats::lag()\n",
      "✖ dplyr::last()      masks data.table::last()\n",
      "✖ purrr::transpose() masks data.table::transpose()\n",
      "Loading required package: igraph\n",
      "\n",
      "Attaching package: ‘igraph’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    as_data_frame, groups, union\n",
      "\n",
      "The following objects are masked from ‘package:purrr’:\n",
      "\n",
      "    compose, simplify\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    crossing\n",
      "\n",
      "The following object is masked from ‘package:tibble’:\n",
      "\n",
      "    as_data_frame\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    decompose, spectrum\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    union\n",
      "\n",
      "Warning message:\n",
      "“package ‘lhs’ was built under R version 3.6.3”"
     ]
    }
   ],
   "source": [
    "rm(list=ls())\n",
    "\n",
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(rJava)\n",
    "library(RNetLogo)\n",
    "library(lhs)\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f2ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘plotly’\n",
      "\n",
      "The following object is masked from ‘package:igraph’:\n",
      "\n",
      "    groups\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    last_plot\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    filter\n",
      "\n",
      "The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    layout\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n",
      "Loading required package: gridExtra\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "The following object is masked from ‘package:randomForest’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder.path = \"/Users/ecemnaz.yildiz/Documents/Personal/Thesis/\"\n",
    "source(paste0(folder.path,\"ThesisSetupCode.r\"))\n",
    "\n",
    "Is_Headless <- 1\n",
    "nl.model <- \"info_cascade_update_TDP_JPF_2020\"\n",
    "\n",
    "nl.path <- \"/Users/ecemnaz.yildiz/Documents/NetLogo 6.0.4/Java\"\n",
    "folder.path = \"/Users/ecemnaz.yildiz/Documents/Personal/Thesis/\"\n",
    "\n",
    "model.path <- paste0(folder.path, nl.model, \".nlogo\")\n",
    "\n",
    "if (Is_Headless == 0) {\n",
    "    NLStart(nl.path, gui = TRUE, nl.jarname = \"netlogo-6.0.4.jar\")\n",
    "    NLLoadModel(model.path)\n",
    "} else {\n",
    "    NLStart(nl.path, gui = FALSE, nl.jarname = \"netlogo-6.0.4.jar\", nl.obj = nl.model)\n",
    "    NLLoadModel(model.path, nl.obj = nl.model)\n",
    "}\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac15d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.type = \"info_cascade_update\"\n",
    "# the path of data folder\n",
    "data.path = paste0(folder.path,\"data/\")\n",
    "# the path for outputs to be record\n",
    "output.folder = paste0(\"outputs_AdS_cvar_TimeTrack_\",model.type,\"_\",Sys.Date())\n",
    "dir.create(file.path(folder.path, output.folder), showWarnings = FALSE)\n",
    "\n",
    "outputs.path = paste0(folder.path,output.folder,\"/\")\n",
    "\n",
    "# Read Me File to keep info about the output folder\n",
    "ReadMe = paste0(outputs.path,\"ReadMe_\",model.type,\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc78c8d",
   "metadata": {},
   "source": [
    "## Model Parameters & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2301c4b",
   "metadata": {},
   "source": [
    "### Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9abc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Parameters ####\n",
    "## Set model parameters Number of replications for each instance\n",
    "nofrep = 30\n",
    "\n",
    "feature_names = c(\"ground-truth\",\"max_links\"\n",
    "                  ,\"evidence\",\"sc-bel-prop\"\n",
    "                  ,\"prop-likelihood\",\"p-h-given-c\"\n",
    "                  ,\"n_init_believers\",\"learning_rate\"\n",
    "                  ,\"con-threshold\",\"expertise_influence\") \n",
    "    \n",
    "feature_ranges = data.table(  feature   = feature_names\n",
    "                            , min_range = c(0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "                            , max_range = c(1, 500, 100, 5, 1, 1, 100, 1, 1, 1)\n",
    "                           )\n",
    "\n",
    "output_name = c(\"cl-prop-same\")\n",
    "\n",
    "# Number of input parameters of the agent-based model\n",
    "nofparams = length(feature_names)\n",
    "\n",
    "# set RF parameters\n",
    "ntree = 300\n",
    "mtry = 6\n",
    "nperm = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c5111",
   "metadata": {},
   "source": [
    "### Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9c2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User parameters ####\n",
    "error_type = \"RMSE\"  # MAPE, BIAS\n",
    "\n",
    "# choose the uncertainty measure\n",
    "selection_metric <- \"coefvar\"  #, 'range' \n",
    "sample.type = selection_metric\n",
    "\n",
    "# Number of iterations\n",
    "iteration_budget = 11\n",
    "metarep = c(1:1)\n",
    "\n",
    "# Number of instances\n",
    "unlabeled_ins = 30\n",
    "test_ins = c(30) #c(100,200,300,400)\n",
    "train_ins_oneshot = 30\n",
    "train_ins_Ad = 30\n",
    "\n",
    "# Set selection parameters\n",
    "selected_ins = 5  #nofinstancesWillbeSelected in each step\n",
    "\n",
    "# Set elimination parameters\n",
    "h <- 1  # number of variables eliminated in each step\n",
    "\n",
    "seed.focus = c(0)#,1,2,3,4,5,6,7,8,9,20)\n",
    "\n",
    "## to be used in log entries\n",
    "unlabeled.type = \"refresh\"\n",
    "\n",
    "log_entry.sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc953a",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5edb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test Sets ####\n",
    "test_set = data.table()\n",
    "\n",
    "for( t in test_ins){\n",
    "    test_set.name= paste0(data.path,\"test_set\",\"_\",model.type,\"_\",t,\".csv\")\n",
    "    test_set_Sub <- fread(test_set.name)  \n",
    "    \n",
    "    test_set = rbind(test_set, data.table(size = t, test_set_Sub))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2556d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>size</th><th scope=col>ground-truth</th><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>p-h-given-c</th><th scope=col>n_init_believers</th><th scope=col>learning_rate</th><th scope=col>con-threshold</th><th scope=col>expertise_influence</th><th scope=col>output</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>30         </td><td>0.3042768  </td><td> 89.112113 </td><td>70.470940  </td><td>4.9971487  </td><td>0.21415365 </td><td>0.31318558 </td><td>73.333450  </td><td>0.388069845</td><td>0.25251921 </td><td>0.68713367 </td><td>43.63463   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4747389  </td><td>129.149248 </td><td>33.784110  </td><td>1.0882790  </td><td>0.66338854 </td><td>0.69404958 </td><td>26.549282  </td><td>0.529586065</td><td>0.55243245 </td><td>0.21574479 </td><td>47.60641   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.9935258  </td><td>402.767907 </td><td>10.233580  </td><td>2.1787607  </td><td>0.92243732 </td><td>0.20535621 </td><td>49.550760  </td><td>0.549731941</td><td>0.36439035 </td><td>0.21913514 </td><td>45.52467   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.5206539  </td><td> 82.418144 </td><td>66.200631  </td><td>0.2948646  </td><td>0.15308214 </td><td>0.89228352 </td><td>50.357083  </td><td>0.239288878</td><td>0.58056254 </td><td>0.92578717 </td><td>45.60589   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.8432310  </td><td>488.704130 </td><td>73.908265  </td><td>3.1535899  </td><td>0.43870431 </td><td>0.58688183 </td><td>67.234487  </td><td>0.782341848</td><td>0.33147896 </td><td>0.78232558 </td><td>44.08580   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.7233145  </td><td> 17.862386 </td><td>89.873140  </td><td>4.4003518  </td><td>0.99618287 </td><td>0.88094505 </td><td>43.866759  </td><td>0.068812180</td><td>0.60581744 </td><td>0.75735539 </td><td>52.11613   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.6151681  </td><td>412.752611 </td><td>29.606159  </td><td>0.5963210  </td><td>0.44761687 </td><td>0.98306526 </td><td>41.839219  </td><td>0.190624533</td><td>0.02576714 </td><td>0.07967662 </td><td>46.26020   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.7391713  </td><td> 20.572940 </td><td>52.469591  </td><td>1.7078387  </td><td>0.81519827 </td><td>0.32036658 </td><td>45.222689  </td><td>0.043802018</td><td>0.29116257 </td><td>0.71565510 </td><td>50.73674   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4172400  </td><td> 26.537910 </td><td>15.394895  </td><td>4.2969741  </td><td>0.18812904 </td><td>0.81556064 </td><td>66.006938  </td><td>0.123153712</td><td>0.20104112 </td><td>0.96558474 </td><td>45.73909   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.3726250  </td><td>256.745520 </td><td>66.876126  </td><td>3.0851056  </td><td>0.20374490 </td><td>0.59864803 </td><td>20.708671  </td><td>0.377884488</td><td>0.17905255 </td><td>0.75177780 </td><td>48.11829   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.9684221  </td><td>141.313256 </td><td>20.957940  </td><td>0.7979056  </td><td>0.95668522 </td><td>0.86270351 </td><td>47.242841  </td><td>0.442012486</td><td>0.22918642 </td><td>0.94496459 </td><td>45.73459   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.6074081  </td><td>216.783786 </td><td>79.898117  </td><td>0.7247586  </td><td>0.99388221 </td><td>0.33065735 </td><td>56.804264  </td><td>0.789083682</td><td>0.51285411 </td><td>0.70580604 </td><td>44.96351   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4736784  </td><td>249.207487 </td><td>52.265780  </td><td>3.5045693  </td><td>0.56330844 </td><td>0.63978828 </td><td>36.947281  </td><td>0.046631717</td><td>0.07851718 </td><td>0.56020628 </td><td>46.66978   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.8581398  </td><td>  5.941332 </td><td> 8.570885  </td><td>3.0692816  </td><td>0.77818000 </td><td>0.38475187 </td><td>81.616977  </td><td>0.501988798</td><td>0.53757772 </td><td>0.32444171 </td><td>57.58605   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4353518  </td><td>124.742291 </td><td>82.899758  </td><td>0.3400183  </td><td>0.98235495 </td><td>0.26389852 </td><td>34.533888  </td><td>0.092986771</td><td>0.87184426 </td><td>0.57360884 </td><td>46.89245   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.0822696  </td><td> 94.384688 </td><td>34.058945  </td><td>1.6546036  </td><td>0.09313455 </td><td>0.14191936 </td><td>43.623852  </td><td>0.247718807</td><td>0.26449017 </td><td>0.41723208 </td><td>46.11045   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4207948  </td><td> 73.472262 </td><td>12.992125  </td><td>3.3526248  </td><td>0.38437853 </td><td>0.74333837 </td><td>30.813266  </td><td>0.288122695</td><td>0.56478839 </td><td>0.09623399 </td><td>47.28241   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.1749127  </td><td>205.930562 </td><td>10.472279  </td><td>1.1503605  </td><td>0.58595874 </td><td>0.92618484 </td><td>13.804640  </td><td>0.689026008</td><td>0.07422151 </td><td>0.05960891 </td><td>48.74161   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.6181256  </td><td>146.734161 </td><td>42.028349  </td><td>4.6163087  </td><td>0.25500563 </td><td>0.95336044 </td><td>21.983866  </td><td>0.006391084</td><td>0.31654062 </td><td>0.86038321 </td><td>48.10853   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.3715556  </td><td>130.745869 </td><td>26.966727  </td><td>0.1983503  </td><td>0.76000086 </td><td>0.64663058 </td><td>77.549871  </td><td>0.618781889</td><td>0.12517591 </td><td>0.44145520 </td><td>43.25915   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.2224613  </td><td>355.471987 </td><td>40.963039  </td><td>2.3678835  </td><td>0.30894127 </td><td>0.06318170 </td><td> 4.341185  </td><td>0.880838591</td><td>0.29851887 </td><td>0.34056975 </td><td>49.76119   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.3603528  </td><td>345.555403 </td><td>23.436460  </td><td>0.7717471  </td><td>0.66807605 </td><td>0.99393506 </td><td>42.916233  </td><td>0.303813092</td><td>0.02868927 </td><td>0.19776526 </td><td>46.16468   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.5326615  </td><td>299.901597 </td><td>71.182231  </td><td>4.7287779  </td><td>0.42011349 </td><td>0.72318684 </td><td>21.208180  </td><td>0.786316358</td><td>0.17997520 </td><td>0.48333523 </td><td>48.00443   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.7964695  </td><td>189.740067 </td><td>16.764319  </td><td>4.8907095  </td><td>0.16840550 </td><td>0.37937642 </td><td>94.191537  </td><td>0.174570792</td><td>0.12381809 </td><td>0.91524320 </td><td>41.98081   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.7712677  </td><td> 91.788832 </td><td>77.299467  </td><td>0.9913445  </td><td>0.33266757 </td><td>0.10747200 </td><td> 7.914372  </td><td>0.946020489</td><td>0.01511171 </td><td>0.04837217 </td><td>49.59592   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.9226118  </td><td>320.565165 </td><td>76.416980  </td><td>4.0170991  </td><td>0.10479836 </td><td>0.55243810 </td><td>70.766570  </td><td>0.770229278</td><td>0.34365028 </td><td>0.33933058 </td><td>43.87060   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.4665194  </td><td>397.570695 </td><td>97.245745  </td><td>4.2566754  </td><td>0.34711122 </td><td>0.10198079 </td><td>70.289951  </td><td>0.526290378</td><td>0.33711828 </td><td>0.28104724 </td><td>43.85840   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.9964133  </td><td>245.334573 </td><td>35.219010  </td><td>1.5793719  </td><td>0.16493433 </td><td>0.62701677 </td><td>79.201951  </td><td>0.516147379</td><td>0.82575993 </td><td>0.11227138 </td><td>43.14313   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.2139196  </td><td> 29.673447 </td><td>84.308254  </td><td>1.8987250  </td><td>0.63554406 </td><td>0.11121458 </td><td>34.542538  </td><td>0.848261417</td><td>0.89155801 </td><td>0.20589420 </td><td>48.21098   </td></tr>\n",
       "\t<tr><td>30         </td><td>0.2147626  </td><td> 73.171010 </td><td> 4.510093  </td><td>1.3810748  </td><td>0.87765699 </td><td>0.01125575 </td><td>12.508616  </td><td>0.758276936</td><td>0.20119084 </td><td>0.38810039 </td><td>51.02797   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " size & ground-truth & max\\_links & evidence & sc-bel-prop & prop-likelihood & p-h-given-c & n\\_init\\_believers & learning\\_rate & con-threshold & expertise\\_influence & output\\\\\n",
       "\\hline\n",
       "\t 30          & 0.3042768   &  89.112113  & 70.470940   & 4.9971487   & 0.21415365  & 0.31318558  & 73.333450   & 0.388069845 & 0.25251921  & 0.68713367  & 43.63463   \\\\\n",
       "\t 30          & 0.4747389   & 129.149248  & 33.784110   & 1.0882790   & 0.66338854  & 0.69404958  & 26.549282   & 0.529586065 & 0.55243245  & 0.21574479  & 47.60641   \\\\\n",
       "\t 30          & 0.9935258   & 402.767907  & 10.233580   & 2.1787607   & 0.92243732  & 0.20535621  & 49.550760   & 0.549731941 & 0.36439035  & 0.21913514  & 45.52467   \\\\\n",
       "\t 30          & 0.5206539   &  82.418144  & 66.200631   & 0.2948646   & 0.15308214  & 0.89228352  & 50.357083   & 0.239288878 & 0.58056254  & 0.92578717  & 45.60589   \\\\\n",
       "\t 30          & 0.8432310   & 488.704130  & 73.908265   & 3.1535899   & 0.43870431  & 0.58688183  & 67.234487   & 0.782341848 & 0.33147896  & 0.78232558  & 44.08580   \\\\\n",
       "\t 30          & 0.7233145   &  17.862386  & 89.873140   & 4.4003518   & 0.99618287  & 0.88094505  & 43.866759   & 0.068812180 & 0.60581744  & 0.75735539  & 52.11613   \\\\\n",
       "\t 30          & 0.6151681   & 412.752611  & 29.606159   & 0.5963210   & 0.44761687  & 0.98306526  & 41.839219   & 0.190624533 & 0.02576714  & 0.07967662  & 46.26020   \\\\\n",
       "\t 30          & 0.7391713   &  20.572940  & 52.469591   & 1.7078387   & 0.81519827  & 0.32036658  & 45.222689   & 0.043802018 & 0.29116257  & 0.71565510  & 50.73674   \\\\\n",
       "\t 30          & 0.4172400   &  26.537910  & 15.394895   & 4.2969741   & 0.18812904  & 0.81556064  & 66.006938   & 0.123153712 & 0.20104112  & 0.96558474  & 45.73909   \\\\\n",
       "\t 30          & 0.3726250   & 256.745520  & 66.876126   & 3.0851056   & 0.20374490  & 0.59864803  & 20.708671   & 0.377884488 & 0.17905255  & 0.75177780  & 48.11829   \\\\\n",
       "\t 30          & 0.9684221   & 141.313256  & 20.957940   & 0.7979056   & 0.95668522  & 0.86270351  & 47.242841   & 0.442012486 & 0.22918642  & 0.94496459  & 45.73459   \\\\\n",
       "\t 30          & 0.6074081   & 216.783786  & 79.898117   & 0.7247586   & 0.99388221  & 0.33065735  & 56.804264   & 0.789083682 & 0.51285411  & 0.70580604  & 44.96351   \\\\\n",
       "\t 30          & 0.4736784   & 249.207487  & 52.265780   & 3.5045693   & 0.56330844  & 0.63978828  & 36.947281   & 0.046631717 & 0.07851718  & 0.56020628  & 46.66978   \\\\\n",
       "\t 30          & 0.8581398   &   5.941332  &  8.570885   & 3.0692816   & 0.77818000  & 0.38475187  & 81.616977   & 0.501988798 & 0.53757772  & 0.32444171  & 57.58605   \\\\\n",
       "\t 30          & 0.4353518   & 124.742291  & 82.899758   & 0.3400183   & 0.98235495  & 0.26389852  & 34.533888   & 0.092986771 & 0.87184426  & 0.57360884  & 46.89245   \\\\\n",
       "\t 30          & 0.0822696   &  94.384688  & 34.058945   & 1.6546036   & 0.09313455  & 0.14191936  & 43.623852   & 0.247718807 & 0.26449017  & 0.41723208  & 46.11045   \\\\\n",
       "\t 30          & 0.4207948   &  73.472262  & 12.992125   & 3.3526248   & 0.38437853  & 0.74333837  & 30.813266   & 0.288122695 & 0.56478839  & 0.09623399  & 47.28241   \\\\\n",
       "\t 30          & 0.1749127   & 205.930562  & 10.472279   & 1.1503605   & 0.58595874  & 0.92618484  & 13.804640   & 0.689026008 & 0.07422151  & 0.05960891  & 48.74161   \\\\\n",
       "\t 30          & 0.6181256   & 146.734161  & 42.028349   & 4.6163087   & 0.25500563  & 0.95336044  & 21.983866   & 0.006391084 & 0.31654062  & 0.86038321  & 48.10853   \\\\\n",
       "\t 30          & 0.3715556   & 130.745869  & 26.966727   & 0.1983503   & 0.76000086  & 0.64663058  & 77.549871   & 0.618781889 & 0.12517591  & 0.44145520  & 43.25915   \\\\\n",
       "\t 30          & 0.2224613   & 355.471987  & 40.963039   & 2.3678835   & 0.30894127  & 0.06318170  &  4.341185   & 0.880838591 & 0.29851887  & 0.34056975  & 49.76119   \\\\\n",
       "\t 30          & 0.3603528   & 345.555403  & 23.436460   & 0.7717471   & 0.66807605  & 0.99393506  & 42.916233   & 0.303813092 & 0.02868927  & 0.19776526  & 46.16468   \\\\\n",
       "\t 30          & 0.5326615   & 299.901597  & 71.182231   & 4.7287779   & 0.42011349  & 0.72318684  & 21.208180   & 0.786316358 & 0.17997520  & 0.48333523  & 48.00443   \\\\\n",
       "\t 30          & 0.7964695   & 189.740067  & 16.764319   & 4.8907095   & 0.16840550  & 0.37937642  & 94.191537   & 0.174570792 & 0.12381809  & 0.91524320  & 41.98081   \\\\\n",
       "\t 30          & 0.7712677   &  91.788832  & 77.299467   & 0.9913445   & 0.33266757  & 0.10747200  &  7.914372   & 0.946020489 & 0.01511171  & 0.04837217  & 49.59592   \\\\\n",
       "\t 30          & 0.9226118   & 320.565165  & 76.416980   & 4.0170991   & 0.10479836  & 0.55243810  & 70.766570   & 0.770229278 & 0.34365028  & 0.33933058  & 43.87060   \\\\\n",
       "\t 30          & 0.4665194   & 397.570695  & 97.245745   & 4.2566754   & 0.34711122  & 0.10198079  & 70.289951   & 0.526290378 & 0.33711828  & 0.28104724  & 43.85840   \\\\\n",
       "\t 30          & 0.9964133   & 245.334573  & 35.219010   & 1.5793719   & 0.16493433  & 0.62701677  & 79.201951   & 0.516147379 & 0.82575993  & 0.11227138  & 43.14313   \\\\\n",
       "\t 30          & 0.2139196   &  29.673447  & 84.308254   & 1.8987250   & 0.63554406  & 0.11121458  & 34.542538   & 0.848261417 & 0.89155801  & 0.20589420  & 48.21098   \\\\\n",
       "\t 30          & 0.2147626   &  73.171010  &  4.510093   & 1.3810748   & 0.87765699  & 0.01125575  & 12.508616   & 0.758276936 & 0.20119084  & 0.38810039  & 51.02797   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| size | ground-truth | max_links | evidence | sc-bel-prop | prop-likelihood | p-h-given-c | n_init_believers | learning_rate | con-threshold | expertise_influence | output |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 30          | 0.3042768   |  89.112113  | 70.470940   | 4.9971487   | 0.21415365  | 0.31318558  | 73.333450   | 0.388069845 | 0.25251921  | 0.68713367  | 43.63463    |\n",
       "| 30          | 0.4747389   | 129.149248  | 33.784110   | 1.0882790   | 0.66338854  | 0.69404958  | 26.549282   | 0.529586065 | 0.55243245  | 0.21574479  | 47.60641    |\n",
       "| 30          | 0.9935258   | 402.767907  | 10.233580   | 2.1787607   | 0.92243732  | 0.20535621  | 49.550760   | 0.549731941 | 0.36439035  | 0.21913514  | 45.52467    |\n",
       "| 30          | 0.5206539   |  82.418144  | 66.200631   | 0.2948646   | 0.15308214  | 0.89228352  | 50.357083   | 0.239288878 | 0.58056254  | 0.92578717  | 45.60589    |\n",
       "| 30          | 0.8432310   | 488.704130  | 73.908265   | 3.1535899   | 0.43870431  | 0.58688183  | 67.234487   | 0.782341848 | 0.33147896  | 0.78232558  | 44.08580    |\n",
       "| 30          | 0.7233145   |  17.862386  | 89.873140   | 4.4003518   | 0.99618287  | 0.88094505  | 43.866759   | 0.068812180 | 0.60581744  | 0.75735539  | 52.11613    |\n",
       "| 30          | 0.6151681   | 412.752611  | 29.606159   | 0.5963210   | 0.44761687  | 0.98306526  | 41.839219   | 0.190624533 | 0.02576714  | 0.07967662  | 46.26020    |\n",
       "| 30          | 0.7391713   |  20.572940  | 52.469591   | 1.7078387   | 0.81519827  | 0.32036658  | 45.222689   | 0.043802018 | 0.29116257  | 0.71565510  | 50.73674    |\n",
       "| 30          | 0.4172400   |  26.537910  | 15.394895   | 4.2969741   | 0.18812904  | 0.81556064  | 66.006938   | 0.123153712 | 0.20104112  | 0.96558474  | 45.73909    |\n",
       "| 30          | 0.3726250   | 256.745520  | 66.876126   | 3.0851056   | 0.20374490  | 0.59864803  | 20.708671   | 0.377884488 | 0.17905255  | 0.75177780  | 48.11829    |\n",
       "| 30          | 0.9684221   | 141.313256  | 20.957940   | 0.7979056   | 0.95668522  | 0.86270351  | 47.242841   | 0.442012486 | 0.22918642  | 0.94496459  | 45.73459    |\n",
       "| 30          | 0.6074081   | 216.783786  | 79.898117   | 0.7247586   | 0.99388221  | 0.33065735  | 56.804264   | 0.789083682 | 0.51285411  | 0.70580604  | 44.96351    |\n",
       "| 30          | 0.4736784   | 249.207487  | 52.265780   | 3.5045693   | 0.56330844  | 0.63978828  | 36.947281   | 0.046631717 | 0.07851718  | 0.56020628  | 46.66978    |\n",
       "| 30          | 0.8581398   |   5.941332  |  8.570885   | 3.0692816   | 0.77818000  | 0.38475187  | 81.616977   | 0.501988798 | 0.53757772  | 0.32444171  | 57.58605    |\n",
       "| 30          | 0.4353518   | 124.742291  | 82.899758   | 0.3400183   | 0.98235495  | 0.26389852  | 34.533888   | 0.092986771 | 0.87184426  | 0.57360884  | 46.89245    |\n",
       "| 30          | 0.0822696   |  94.384688  | 34.058945   | 1.6546036   | 0.09313455  | 0.14191936  | 43.623852   | 0.247718807 | 0.26449017  | 0.41723208  | 46.11045    |\n",
       "| 30          | 0.4207948   |  73.472262  | 12.992125   | 3.3526248   | 0.38437853  | 0.74333837  | 30.813266   | 0.288122695 | 0.56478839  | 0.09623399  | 47.28241    |\n",
       "| 30          | 0.1749127   | 205.930562  | 10.472279   | 1.1503605   | 0.58595874  | 0.92618484  | 13.804640   | 0.689026008 | 0.07422151  | 0.05960891  | 48.74161    |\n",
       "| 30          | 0.6181256   | 146.734161  | 42.028349   | 4.6163087   | 0.25500563  | 0.95336044  | 21.983866   | 0.006391084 | 0.31654062  | 0.86038321  | 48.10853    |\n",
       "| 30          | 0.3715556   | 130.745869  | 26.966727   | 0.1983503   | 0.76000086  | 0.64663058  | 77.549871   | 0.618781889 | 0.12517591  | 0.44145520  | 43.25915    |\n",
       "| 30          | 0.2224613   | 355.471987  | 40.963039   | 2.3678835   | 0.30894127  | 0.06318170  |  4.341185   | 0.880838591 | 0.29851887  | 0.34056975  | 49.76119    |\n",
       "| 30          | 0.3603528   | 345.555403  | 23.436460   | 0.7717471   | 0.66807605  | 0.99393506  | 42.916233   | 0.303813092 | 0.02868927  | 0.19776526  | 46.16468    |\n",
       "| 30          | 0.5326615   | 299.901597  | 71.182231   | 4.7287779   | 0.42011349  | 0.72318684  | 21.208180   | 0.786316358 | 0.17997520  | 0.48333523  | 48.00443    |\n",
       "| 30          | 0.7964695   | 189.740067  | 16.764319   | 4.8907095   | 0.16840550  | 0.37937642  | 94.191537   | 0.174570792 | 0.12381809  | 0.91524320  | 41.98081    |\n",
       "| 30          | 0.7712677   |  91.788832  | 77.299467   | 0.9913445   | 0.33266757  | 0.10747200  |  7.914372   | 0.946020489 | 0.01511171  | 0.04837217  | 49.59592    |\n",
       "| 30          | 0.9226118   | 320.565165  | 76.416980   | 4.0170991   | 0.10479836  | 0.55243810  | 70.766570   | 0.770229278 | 0.34365028  | 0.33933058  | 43.87060    |\n",
       "| 30          | 0.4665194   | 397.570695  | 97.245745   | 4.2566754   | 0.34711122  | 0.10198079  | 70.289951   | 0.526290378 | 0.33711828  | 0.28104724  | 43.85840    |\n",
       "| 30          | 0.9964133   | 245.334573  | 35.219010   | 1.5793719   | 0.16493433  | 0.62701677  | 79.201951   | 0.516147379 | 0.82575993  | 0.11227138  | 43.14313    |\n",
       "| 30          | 0.2139196   |  29.673447  | 84.308254   | 1.8987250   | 0.63554406  | 0.11121458  | 34.542538   | 0.848261417 | 0.89155801  | 0.20589420  | 48.21098    |\n",
       "| 30          | 0.2147626   |  73.171010  |  4.510093   | 1.3810748   | 0.87765699  | 0.01125575  | 12.508616   | 0.758276936 | 0.20119084  | 0.38810039  | 51.02797    |\n",
       "\n"
      ],
      "text/plain": [
       "   size ground-truth max_links  evidence  sc-bel-prop prop-likelihood\n",
       "1  30   0.3042768     89.112113 70.470940 4.9971487   0.21415365     \n",
       "2  30   0.4747389    129.149248 33.784110 1.0882790   0.66338854     \n",
       "3  30   0.9935258    402.767907 10.233580 2.1787607   0.92243732     \n",
       "4  30   0.5206539     82.418144 66.200631 0.2948646   0.15308214     \n",
       "5  30   0.8432310    488.704130 73.908265 3.1535899   0.43870431     \n",
       "6  30   0.7233145     17.862386 89.873140 4.4003518   0.99618287     \n",
       "7  30   0.6151681    412.752611 29.606159 0.5963210   0.44761687     \n",
       "8  30   0.7391713     20.572940 52.469591 1.7078387   0.81519827     \n",
       "9  30   0.4172400     26.537910 15.394895 4.2969741   0.18812904     \n",
       "10 30   0.3726250    256.745520 66.876126 3.0851056   0.20374490     \n",
       "11 30   0.9684221    141.313256 20.957940 0.7979056   0.95668522     \n",
       "12 30   0.6074081    216.783786 79.898117 0.7247586   0.99388221     \n",
       "13 30   0.4736784    249.207487 52.265780 3.5045693   0.56330844     \n",
       "14 30   0.8581398      5.941332  8.570885 3.0692816   0.77818000     \n",
       "15 30   0.4353518    124.742291 82.899758 0.3400183   0.98235495     \n",
       "16 30   0.0822696     94.384688 34.058945 1.6546036   0.09313455     \n",
       "17 30   0.4207948     73.472262 12.992125 3.3526248   0.38437853     \n",
       "18 30   0.1749127    205.930562 10.472279 1.1503605   0.58595874     \n",
       "19 30   0.6181256    146.734161 42.028349 4.6163087   0.25500563     \n",
       "20 30   0.3715556    130.745869 26.966727 0.1983503   0.76000086     \n",
       "21 30   0.2224613    355.471987 40.963039 2.3678835   0.30894127     \n",
       "22 30   0.3603528    345.555403 23.436460 0.7717471   0.66807605     \n",
       "23 30   0.5326615    299.901597 71.182231 4.7287779   0.42011349     \n",
       "24 30   0.7964695    189.740067 16.764319 4.8907095   0.16840550     \n",
       "25 30   0.7712677     91.788832 77.299467 0.9913445   0.33266757     \n",
       "26 30   0.9226118    320.565165 76.416980 4.0170991   0.10479836     \n",
       "27 30   0.4665194    397.570695 97.245745 4.2566754   0.34711122     \n",
       "28 30   0.9964133    245.334573 35.219010 1.5793719   0.16493433     \n",
       "29 30   0.2139196     29.673447 84.308254 1.8987250   0.63554406     \n",
       "30 30   0.2147626     73.171010  4.510093 1.3810748   0.87765699     \n",
       "   p-h-given-c n_init_believers learning_rate con-threshold expertise_influence\n",
       "1  0.31318558  73.333450        0.388069845   0.25251921    0.68713367         \n",
       "2  0.69404958  26.549282        0.529586065   0.55243245    0.21574479         \n",
       "3  0.20535621  49.550760        0.549731941   0.36439035    0.21913514         \n",
       "4  0.89228352  50.357083        0.239288878   0.58056254    0.92578717         \n",
       "5  0.58688183  67.234487        0.782341848   0.33147896    0.78232558         \n",
       "6  0.88094505  43.866759        0.068812180   0.60581744    0.75735539         \n",
       "7  0.98306526  41.839219        0.190624533   0.02576714    0.07967662         \n",
       "8  0.32036658  45.222689        0.043802018   0.29116257    0.71565510         \n",
       "9  0.81556064  66.006938        0.123153712   0.20104112    0.96558474         \n",
       "10 0.59864803  20.708671        0.377884488   0.17905255    0.75177780         \n",
       "11 0.86270351  47.242841        0.442012486   0.22918642    0.94496459         \n",
       "12 0.33065735  56.804264        0.789083682   0.51285411    0.70580604         \n",
       "13 0.63978828  36.947281        0.046631717   0.07851718    0.56020628         \n",
       "14 0.38475187  81.616977        0.501988798   0.53757772    0.32444171         \n",
       "15 0.26389852  34.533888        0.092986771   0.87184426    0.57360884         \n",
       "16 0.14191936  43.623852        0.247718807   0.26449017    0.41723208         \n",
       "17 0.74333837  30.813266        0.288122695   0.56478839    0.09623399         \n",
       "18 0.92618484  13.804640        0.689026008   0.07422151    0.05960891         \n",
       "19 0.95336044  21.983866        0.006391084   0.31654062    0.86038321         \n",
       "20 0.64663058  77.549871        0.618781889   0.12517591    0.44145520         \n",
       "21 0.06318170   4.341185        0.880838591   0.29851887    0.34056975         \n",
       "22 0.99393506  42.916233        0.303813092   0.02868927    0.19776526         \n",
       "23 0.72318684  21.208180        0.786316358   0.17997520    0.48333523         \n",
       "24 0.37937642  94.191537        0.174570792   0.12381809    0.91524320         \n",
       "25 0.10747200   7.914372        0.946020489   0.01511171    0.04837217         \n",
       "26 0.55243810  70.766570        0.770229278   0.34365028    0.33933058         \n",
       "27 0.10198079  70.289951        0.526290378   0.33711828    0.28104724         \n",
       "28 0.62701677  79.201951        0.516147379   0.82575993    0.11227138         \n",
       "29 0.11121458  34.542538        0.848261417   0.89155801    0.20589420         \n",
       "30 0.01125575  12.508616        0.758276936   0.20119084    0.38810039         \n",
       "   output  \n",
       "1  43.63463\n",
       "2  47.60641\n",
       "3  45.52467\n",
       "4  45.60589\n",
       "5  44.08580\n",
       "6  52.11613\n",
       "7  46.26020\n",
       "8  50.73674\n",
       "9  45.73909\n",
       "10 48.11829\n",
       "11 45.73459\n",
       "12 44.96351\n",
       "13 46.66978\n",
       "14 57.58605\n",
       "15 46.89245\n",
       "16 46.11045\n",
       "17 47.28241\n",
       "18 48.74161\n",
       "19 48.10853\n",
       "20 43.25915\n",
       "21 49.76119\n",
       "22 46.16468\n",
       "23 48.00443\n",
       "24 41.98081\n",
       "25 49.59592\n",
       "26 43.87060\n",
       "27 43.85840\n",
       "28 43.14313\n",
       "29 48.21098\n",
       "30 51.02797"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf8b34",
   "metadata": {},
   "source": [
    "## One-Shot Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d8ab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_data = upload_training_set(model.type,seed.focus,train_ins_oneshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59aa3b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ground-truth</th><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>p-h-given-c</th><th scope=col>n_init_believers</th><th scope=col>learning_rate</th><th scope=col>con-threshold</th><th scope=col>expertise_influence</th><th scope=col>output</th><th scope=col>seed</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.47601769 </td><td>309.80882  </td><td>83.549839  </td><td>2.68861870 </td><td>0.65168982 </td><td>0.63354368 </td><td>57.8611225 </td><td>0.54632148 </td><td>0.32160454 </td><td>0.541202792</td><td>44.91015   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.35672834 </td><td>317.30518  </td><td>63.106664  </td><td>2.22335191 </td><td>0.90763209 </td><td>0.56252703 </td><td>76.1547683 </td><td>0.47592333 </td><td>0.46328014 </td><td>0.524350752</td><td>43.35827   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.71638853 </td><td> 80.00353  </td><td>58.991850  </td><td>2.40930323 </td><td>0.68969464 </td><td>0.74683716 </td><td>43.1984348 </td><td>0.50098188 </td><td>0.38406676 </td><td>0.475940387</td><td>46.13721   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.38894489 </td><td>268.21950  </td><td>83.062600  </td><td>3.66248914 </td><td>0.62880975 </td><td>0.60311785 </td><td>34.5217967 </td><td>0.23294625 </td><td>0.70413467 </td><td>0.239238373</td><td>46.87023   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.50152214 </td><td>376.17234  </td><td>68.169001  </td><td>2.96416124 </td><td>0.50981017 </td><td>0.86337559 </td><td>16.2605515 </td><td>0.77409926 </td><td>0.60234462 </td><td>0.610609240</td><td>48.47599   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.40541209 </td><td>263.49302  </td><td>47.911606  </td><td>1.78630814 </td><td>0.26884695 </td><td>0.45373039 </td><td>77.0902000 </td><td>0.60907955 </td><td>0.67106358 </td><td>0.441360433</td><td>43.27983   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.44819153 </td><td> 86.80230  </td><td>95.770899  </td><td>3.07717965 </td><td>0.46432791 </td><td>0.26671911 </td><td>96.6376782 </td><td>0.96163352 </td><td>0.80923608 </td><td>0.887869010</td><td>41.82674   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.53651635 </td><td>214.02388  </td><td>63.358828  </td><td>3.30239302 </td><td>0.18843268 </td><td>0.47324912 </td><td>90.6509697 </td><td>0.24464914 </td><td>0.13135634 </td><td>0.795441728</td><td>42.26746   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.07952148 </td><td>217.82492  </td><td>33.549733  </td><td>0.98303740 </td><td>0.78358728 </td><td>0.14660758 </td><td>31.4393862 </td><td>0.44938462 </td><td>0.48563582 </td><td>0.428534337</td><td>47.14763   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.24501809 </td><td> 50.51225  </td><td>74.346076  </td><td>1.62021644 </td><td>0.43220176 </td><td>0.89463243 </td><td>12.5805322 </td><td>0.34182141 </td><td>0.54708903 </td><td>0.205643966</td><td>51.20797   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.63856199 </td><td>459.48145  </td><td>53.224632  </td><td>4.40608682 </td><td>0.47381428 </td><td>0.25799465 </td><td>67.9886207 </td><td>0.68151506 </td><td>0.28799508 </td><td>0.320194715</td><td>44.07850   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.80488346 </td><td>354.04219  </td><td>46.331153  </td><td>0.37218905 </td><td>0.83127037 </td><td>0.95397055 </td><td>21.3105169 </td><td>0.19295557 </td><td>0.52876480 </td><td>0.730052653</td><td>48.02256   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.57630990 </td><td> 15.65373  </td><td>90.351600  </td><td>1.18339279 </td><td>0.54256842 </td><td>0.31927096 </td><td>19.1455078 </td><td>0.85751843 </td><td>0.14812456 </td><td>0.733731810</td><td>59.69117   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.77041118 </td><td>171.30952  </td><td>36.678126  </td><td>4.22830276 </td><td>0.72660916 </td><td>0.50263203 </td><td>44.2274257 </td><td>0.03241952 </td><td>0.25700116 </td><td>0.967152144</td><td>46.02098   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.14513135 </td><td>296.79148  </td><td> 2.711361  </td><td>1.83963641 </td><td>0.36661554 </td><td>0.33612344 </td><td>64.7271505 </td><td>0.87168179 </td><td>0.04355859 </td><td>0.190592471</td><td>44.30201   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.29182177 </td><td>403.67980  </td><td>70.859862  </td><td>3.69643211 </td><td>0.26371119 </td><td>0.42379335 </td><td>47.3906214 </td><td>0.31178857 </td><td>0.74889903 </td><td>0.841312388</td><td>45.74135   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.10346714 </td><td> 43.94761  </td><td>26.782110  </td><td>4.66349926 </td><td>0.58452664 </td><td>0.18292822 </td><td>85.1104706 </td><td>0.58659189 </td><td>0.59300508 </td><td>0.672293694</td><td>42.86651   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.98174934 </td><td>198.10849  </td><td>55.243804  </td><td>0.64423474 </td><td>0.10223249 </td><td>0.20594726 </td><td>72.3019701 </td><td>0.97725222 </td><td>0.79485901 </td><td>0.816627014</td><td>43.72011   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.03950368 </td><td>158.08886  </td><td>86.922408  </td><td>1.07490666 </td><td>0.09893722 </td><td>0.39887169 </td><td>25.3765003 </td><td>0.11726508 </td><td>0.02038889 </td><td>0.153329309</td><td>47.68389   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.85188611 </td><td>476.08661  </td><td>17.821875  </td><td>0.18955886 </td><td>0.95177300 </td><td>0.91359260 </td><td>50.0465866 </td><td>0.93145884 </td><td>0.64156264 </td><td>0.001923862</td><td>45.47320   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.67260541 </td><td>105.51007  </td><td>41.871069  </td><td>1.40523149 </td><td>0.76527629 </td><td>0.67846781 </td><td>82.7361302 </td><td>0.08154482 </td><td>0.98428638 </td><td>0.091779928</td><td>42.86634   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.19755468 </td><td>149.53917  </td><td> 3.911665  </td><td>3.98857254 </td><td>0.30140243 </td><td>0.81580866 </td><td> 3.5105931 </td><td>0.82436122 </td><td>0.21835248 </td><td>0.920309543</td><td>55.39434   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.94938161 </td><td>129.17010  </td><td>77.907751  </td><td>4.00984293 </td><td>0.39423793 </td><td>0.08744255 </td><td> 0.5524448 </td><td>0.41810701 </td><td>0.19107811 </td><td>0.296165683</td><td> 0.00000   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.60850101 </td><td>426.19315  </td><td>11.032080  </td><td>3.44727788 </td><td>0.22759002 </td><td>0.97908562 </td><td>99.7437568 </td><td>0.64141421 </td><td>0.93629280 </td><td>0.592873474</td><td>41.62802   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.03283095 </td><td>435.86283  </td><td> 9.179517  </td><td>4.74314211 </td><td>0.98971872 </td><td>0.03473545 </td><td>55.0380307 </td><td>0.38222864 </td><td>0.88022648 </td><td>0.109845465</td><td>45.07540   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.90598062 </td><td>499.15278  </td><td>15.687057  </td><td>2.03653188 </td><td>0.88119443 </td><td>0.02846176 </td><td>37.1724002 </td><td>0.72339159 </td><td>0.34528693 </td><td>0.939470711</td><td>46.58785   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.88081637 </td><td>347.04184  </td><td>31.592234  </td><td>0.68327497 </td><td>0.05928076 </td><td>0.57435605 </td><td>88.5115510 </td><td>0.13893367 </td><td>0.07742080 </td><td>0.049369550</td><td>42.43369   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.73852995 </td><td> 26.99856  </td><td>23.024449  </td><td>4.87827067 </td><td>0.03138657 </td><td>0.76840357 </td><td>27.4412562 </td><td>0.04733936 </td><td>0.83423326 </td><td>0.635451322</td><td>54.81774   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.21712107 </td><td>391.46709  </td><td>26.035756  </td><td>0.09091459 </td><td>0.16354992 </td><td>0.11234408 </td><td> 9.4471089 </td><td>0.29020407 </td><td>0.92921069 </td><td>0.352597127</td><td>49.16110   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.30147697 </td><td>248.26575  </td><td>98.158734  </td><td>2.60679969 </td><td>0.85002898 </td><td>0.72525885 </td><td>61.7155555 </td><td>0.75383667 </td><td>0.40803055 </td><td>0.387801080</td><td>44.58299   </td><td>0          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " ground-truth & max\\_links & evidence & sc-bel-prop & prop-likelihood & p-h-given-c & n\\_init\\_believers & learning\\_rate & con-threshold & expertise\\_influence & output & seed\\\\\n",
       "\\hline\n",
       "\t 0.47601769  & 309.80882   & 83.549839   & 2.68861870  & 0.65168982  & 0.63354368  & 57.8611225  & 0.54632148  & 0.32160454  & 0.541202792 & 44.91015    & 0          \\\\\n",
       "\t 0.35672834  & 317.30518   & 63.106664   & 2.22335191  & 0.90763209  & 0.56252703  & 76.1547683  & 0.47592333  & 0.46328014  & 0.524350752 & 43.35827    & 0          \\\\\n",
       "\t 0.71638853  &  80.00353   & 58.991850   & 2.40930323  & 0.68969464  & 0.74683716  & 43.1984348  & 0.50098188  & 0.38406676  & 0.475940387 & 46.13721    & 0          \\\\\n",
       "\t 0.38894489  & 268.21950   & 83.062600   & 3.66248914  & 0.62880975  & 0.60311785  & 34.5217967  & 0.23294625  & 0.70413467  & 0.239238373 & 46.87023    & 0          \\\\\n",
       "\t 0.50152214  & 376.17234   & 68.169001   & 2.96416124  & 0.50981017  & 0.86337559  & 16.2605515  & 0.77409926  & 0.60234462  & 0.610609240 & 48.47599    & 0          \\\\\n",
       "\t 0.40541209  & 263.49302   & 47.911606   & 1.78630814  & 0.26884695  & 0.45373039  & 77.0902000  & 0.60907955  & 0.67106358  & 0.441360433 & 43.27983    & 0          \\\\\n",
       "\t 0.44819153  &  86.80230   & 95.770899   & 3.07717965  & 0.46432791  & 0.26671911  & 96.6376782  & 0.96163352  & 0.80923608  & 0.887869010 & 41.82674    & 0          \\\\\n",
       "\t 0.53651635  & 214.02388   & 63.358828   & 3.30239302  & 0.18843268  & 0.47324912  & 90.6509697  & 0.24464914  & 0.13135634  & 0.795441728 & 42.26746    & 0          \\\\\n",
       "\t 0.07952148  & 217.82492   & 33.549733   & 0.98303740  & 0.78358728  & 0.14660758  & 31.4393862  & 0.44938462  & 0.48563582  & 0.428534337 & 47.14763    & 0          \\\\\n",
       "\t 0.24501809  &  50.51225   & 74.346076   & 1.62021644  & 0.43220176  & 0.89463243  & 12.5805322  & 0.34182141  & 0.54708903  & 0.205643966 & 51.20797    & 0          \\\\\n",
       "\t 0.63856199  & 459.48145   & 53.224632   & 4.40608682  & 0.47381428  & 0.25799465  & 67.9886207  & 0.68151506  & 0.28799508  & 0.320194715 & 44.07850    & 0          \\\\\n",
       "\t 0.80488346  & 354.04219   & 46.331153   & 0.37218905  & 0.83127037  & 0.95397055  & 21.3105169  & 0.19295557  & 0.52876480  & 0.730052653 & 48.02256    & 0          \\\\\n",
       "\t 0.57630990  &  15.65373   & 90.351600   & 1.18339279  & 0.54256842  & 0.31927096  & 19.1455078  & 0.85751843  & 0.14812456  & 0.733731810 & 59.69117    & 0          \\\\\n",
       "\t 0.77041118  & 171.30952   & 36.678126   & 4.22830276  & 0.72660916  & 0.50263203  & 44.2274257  & 0.03241952  & 0.25700116  & 0.967152144 & 46.02098    & 0          \\\\\n",
       "\t 0.14513135  & 296.79148   &  2.711361   & 1.83963641  & 0.36661554  & 0.33612344  & 64.7271505  & 0.87168179  & 0.04355859  & 0.190592471 & 44.30201    & 0          \\\\\n",
       "\t 0.29182177  & 403.67980   & 70.859862   & 3.69643211  & 0.26371119  & 0.42379335  & 47.3906214  & 0.31178857  & 0.74889903  & 0.841312388 & 45.74135    & 0          \\\\\n",
       "\t 0.10346714  &  43.94761   & 26.782110   & 4.66349926  & 0.58452664  & 0.18292822  & 85.1104706  & 0.58659189  & 0.59300508  & 0.672293694 & 42.86651    & 0          \\\\\n",
       "\t 0.98174934  & 198.10849   & 55.243804   & 0.64423474  & 0.10223249  & 0.20594726  & 72.3019701  & 0.97725222  & 0.79485901  & 0.816627014 & 43.72011    & 0          \\\\\n",
       "\t 0.03950368  & 158.08886   & 86.922408   & 1.07490666  & 0.09893722  & 0.39887169  & 25.3765003  & 0.11726508  & 0.02038889  & 0.153329309 & 47.68389    & 0          \\\\\n",
       "\t 0.85188611  & 476.08661   & 17.821875   & 0.18955886  & 0.95177300  & 0.91359260  & 50.0465866  & 0.93145884  & 0.64156264  & 0.001923862 & 45.47320    & 0          \\\\\n",
       "\t 0.67260541  & 105.51007   & 41.871069   & 1.40523149  & 0.76527629  & 0.67846781  & 82.7361302  & 0.08154482  & 0.98428638  & 0.091779928 & 42.86634    & 0          \\\\\n",
       "\t 0.19755468  & 149.53917   &  3.911665   & 3.98857254  & 0.30140243  & 0.81580866  &  3.5105931  & 0.82436122  & 0.21835248  & 0.920309543 & 55.39434    & 0          \\\\\n",
       "\t 0.94938161  & 129.17010   & 77.907751   & 4.00984293  & 0.39423793  & 0.08744255  &  0.5524448  & 0.41810701  & 0.19107811  & 0.296165683 &  0.00000    & 0          \\\\\n",
       "\t 0.60850101  & 426.19315   & 11.032080   & 3.44727788  & 0.22759002  & 0.97908562  & 99.7437568  & 0.64141421  & 0.93629280  & 0.592873474 & 41.62802    & 0          \\\\\n",
       "\t 0.03283095  & 435.86283   &  9.179517   & 4.74314211  & 0.98971872  & 0.03473545  & 55.0380307  & 0.38222864  & 0.88022648  & 0.109845465 & 45.07540    & 0          \\\\\n",
       "\t 0.90598062  & 499.15278   & 15.687057   & 2.03653188  & 0.88119443  & 0.02846176  & 37.1724002  & 0.72339159  & 0.34528693  & 0.939470711 & 46.58785    & 0          \\\\\n",
       "\t 0.88081637  & 347.04184   & 31.592234   & 0.68327497  & 0.05928076  & 0.57435605  & 88.5115510  & 0.13893367  & 0.07742080  & 0.049369550 & 42.43369    & 0          \\\\\n",
       "\t 0.73852995  &  26.99856   & 23.024449   & 4.87827067  & 0.03138657  & 0.76840357  & 27.4412562  & 0.04733936  & 0.83423326  & 0.635451322 & 54.81774    & 0          \\\\\n",
       "\t 0.21712107  & 391.46709   & 26.035756   & 0.09091459  & 0.16354992  & 0.11234408  &  9.4471089  & 0.29020407  & 0.92921069  & 0.352597127 & 49.16110    & 0          \\\\\n",
       "\t 0.30147697  & 248.26575   & 98.158734   & 2.60679969  & 0.85002898  & 0.72525885  & 61.7155555  & 0.75383667  & 0.40803055  & 0.387801080 & 44.58299    & 0          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| ground-truth | max_links | evidence | sc-bel-prop | prop-likelihood | p-h-given-c | n_init_believers | learning_rate | con-threshold | expertise_influence | output | seed |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.47601769  | 309.80882   | 83.549839   | 2.68861870  | 0.65168982  | 0.63354368  | 57.8611225  | 0.54632148  | 0.32160454  | 0.541202792 | 44.91015    | 0           |\n",
       "| 0.35672834  | 317.30518   | 63.106664   | 2.22335191  | 0.90763209  | 0.56252703  | 76.1547683  | 0.47592333  | 0.46328014  | 0.524350752 | 43.35827    | 0           |\n",
       "| 0.71638853  |  80.00353   | 58.991850   | 2.40930323  | 0.68969464  | 0.74683716  | 43.1984348  | 0.50098188  | 0.38406676  | 0.475940387 | 46.13721    | 0           |\n",
       "| 0.38894489  | 268.21950   | 83.062600   | 3.66248914  | 0.62880975  | 0.60311785  | 34.5217967  | 0.23294625  | 0.70413467  | 0.239238373 | 46.87023    | 0           |\n",
       "| 0.50152214  | 376.17234   | 68.169001   | 2.96416124  | 0.50981017  | 0.86337559  | 16.2605515  | 0.77409926  | 0.60234462  | 0.610609240 | 48.47599    | 0           |\n",
       "| 0.40541209  | 263.49302   | 47.911606   | 1.78630814  | 0.26884695  | 0.45373039  | 77.0902000  | 0.60907955  | 0.67106358  | 0.441360433 | 43.27983    | 0           |\n",
       "| 0.44819153  |  86.80230   | 95.770899   | 3.07717965  | 0.46432791  | 0.26671911  | 96.6376782  | 0.96163352  | 0.80923608  | 0.887869010 | 41.82674    | 0           |\n",
       "| 0.53651635  | 214.02388   | 63.358828   | 3.30239302  | 0.18843268  | 0.47324912  | 90.6509697  | 0.24464914  | 0.13135634  | 0.795441728 | 42.26746    | 0           |\n",
       "| 0.07952148  | 217.82492   | 33.549733   | 0.98303740  | 0.78358728  | 0.14660758  | 31.4393862  | 0.44938462  | 0.48563582  | 0.428534337 | 47.14763    | 0           |\n",
       "| 0.24501809  |  50.51225   | 74.346076   | 1.62021644  | 0.43220176  | 0.89463243  | 12.5805322  | 0.34182141  | 0.54708903  | 0.205643966 | 51.20797    | 0           |\n",
       "| 0.63856199  | 459.48145   | 53.224632   | 4.40608682  | 0.47381428  | 0.25799465  | 67.9886207  | 0.68151506  | 0.28799508  | 0.320194715 | 44.07850    | 0           |\n",
       "| 0.80488346  | 354.04219   | 46.331153   | 0.37218905  | 0.83127037  | 0.95397055  | 21.3105169  | 0.19295557  | 0.52876480  | 0.730052653 | 48.02256    | 0           |\n",
       "| 0.57630990  |  15.65373   | 90.351600   | 1.18339279  | 0.54256842  | 0.31927096  | 19.1455078  | 0.85751843  | 0.14812456  | 0.733731810 | 59.69117    | 0           |\n",
       "| 0.77041118  | 171.30952   | 36.678126   | 4.22830276  | 0.72660916  | 0.50263203  | 44.2274257  | 0.03241952  | 0.25700116  | 0.967152144 | 46.02098    | 0           |\n",
       "| 0.14513135  | 296.79148   |  2.711361   | 1.83963641  | 0.36661554  | 0.33612344  | 64.7271505  | 0.87168179  | 0.04355859  | 0.190592471 | 44.30201    | 0           |\n",
       "| 0.29182177  | 403.67980   | 70.859862   | 3.69643211  | 0.26371119  | 0.42379335  | 47.3906214  | 0.31178857  | 0.74889903  | 0.841312388 | 45.74135    | 0           |\n",
       "| 0.10346714  |  43.94761   | 26.782110   | 4.66349926  | 0.58452664  | 0.18292822  | 85.1104706  | 0.58659189  | 0.59300508  | 0.672293694 | 42.86651    | 0           |\n",
       "| 0.98174934  | 198.10849   | 55.243804   | 0.64423474  | 0.10223249  | 0.20594726  | 72.3019701  | 0.97725222  | 0.79485901  | 0.816627014 | 43.72011    | 0           |\n",
       "| 0.03950368  | 158.08886   | 86.922408   | 1.07490666  | 0.09893722  | 0.39887169  | 25.3765003  | 0.11726508  | 0.02038889  | 0.153329309 | 47.68389    | 0           |\n",
       "| 0.85188611  | 476.08661   | 17.821875   | 0.18955886  | 0.95177300  | 0.91359260  | 50.0465866  | 0.93145884  | 0.64156264  | 0.001923862 | 45.47320    | 0           |\n",
       "| 0.67260541  | 105.51007   | 41.871069   | 1.40523149  | 0.76527629  | 0.67846781  | 82.7361302  | 0.08154482  | 0.98428638  | 0.091779928 | 42.86634    | 0           |\n",
       "| 0.19755468  | 149.53917   |  3.911665   | 3.98857254  | 0.30140243  | 0.81580866  |  3.5105931  | 0.82436122  | 0.21835248  | 0.920309543 | 55.39434    | 0           |\n",
       "| 0.94938161  | 129.17010   | 77.907751   | 4.00984293  | 0.39423793  | 0.08744255  |  0.5524448  | 0.41810701  | 0.19107811  | 0.296165683 |  0.00000    | 0           |\n",
       "| 0.60850101  | 426.19315   | 11.032080   | 3.44727788  | 0.22759002  | 0.97908562  | 99.7437568  | 0.64141421  | 0.93629280  | 0.592873474 | 41.62802    | 0           |\n",
       "| 0.03283095  | 435.86283   |  9.179517   | 4.74314211  | 0.98971872  | 0.03473545  | 55.0380307  | 0.38222864  | 0.88022648  | 0.109845465 | 45.07540    | 0           |\n",
       "| 0.90598062  | 499.15278   | 15.687057   | 2.03653188  | 0.88119443  | 0.02846176  | 37.1724002  | 0.72339159  | 0.34528693  | 0.939470711 | 46.58785    | 0           |\n",
       "| 0.88081637  | 347.04184   | 31.592234   | 0.68327497  | 0.05928076  | 0.57435605  | 88.5115510  | 0.13893367  | 0.07742080  | 0.049369550 | 42.43369    | 0           |\n",
       "| 0.73852995  |  26.99856   | 23.024449   | 4.87827067  | 0.03138657  | 0.76840357  | 27.4412562  | 0.04733936  | 0.83423326  | 0.635451322 | 54.81774    | 0           |\n",
       "| 0.21712107  | 391.46709   | 26.035756   | 0.09091459  | 0.16354992  | 0.11234408  |  9.4471089  | 0.29020407  | 0.92921069  | 0.352597127 | 49.16110    | 0           |\n",
       "| 0.30147697  | 248.26575   | 98.158734   | 2.60679969  | 0.85002898  | 0.72525885  | 61.7155555  | 0.75383667  | 0.40803055  | 0.387801080 | 44.58299    | 0           |\n",
       "\n"
      ],
      "text/plain": [
       "   ground-truth max_links evidence  sc-bel-prop prop-likelihood p-h-given-c\n",
       "1  0.47601769   309.80882 83.549839 2.68861870  0.65168982      0.63354368 \n",
       "2  0.35672834   317.30518 63.106664 2.22335191  0.90763209      0.56252703 \n",
       "3  0.71638853    80.00353 58.991850 2.40930323  0.68969464      0.74683716 \n",
       "4  0.38894489   268.21950 83.062600 3.66248914  0.62880975      0.60311785 \n",
       "5  0.50152214   376.17234 68.169001 2.96416124  0.50981017      0.86337559 \n",
       "6  0.40541209   263.49302 47.911606 1.78630814  0.26884695      0.45373039 \n",
       "7  0.44819153    86.80230 95.770899 3.07717965  0.46432791      0.26671911 \n",
       "8  0.53651635   214.02388 63.358828 3.30239302  0.18843268      0.47324912 \n",
       "9  0.07952148   217.82492 33.549733 0.98303740  0.78358728      0.14660758 \n",
       "10 0.24501809    50.51225 74.346076 1.62021644  0.43220176      0.89463243 \n",
       "11 0.63856199   459.48145 53.224632 4.40608682  0.47381428      0.25799465 \n",
       "12 0.80488346   354.04219 46.331153 0.37218905  0.83127037      0.95397055 \n",
       "13 0.57630990    15.65373 90.351600 1.18339279  0.54256842      0.31927096 \n",
       "14 0.77041118   171.30952 36.678126 4.22830276  0.72660916      0.50263203 \n",
       "15 0.14513135   296.79148  2.711361 1.83963641  0.36661554      0.33612344 \n",
       "16 0.29182177   403.67980 70.859862 3.69643211  0.26371119      0.42379335 \n",
       "17 0.10346714    43.94761 26.782110 4.66349926  0.58452664      0.18292822 \n",
       "18 0.98174934   198.10849 55.243804 0.64423474  0.10223249      0.20594726 \n",
       "19 0.03950368   158.08886 86.922408 1.07490666  0.09893722      0.39887169 \n",
       "20 0.85188611   476.08661 17.821875 0.18955886  0.95177300      0.91359260 \n",
       "21 0.67260541   105.51007 41.871069 1.40523149  0.76527629      0.67846781 \n",
       "22 0.19755468   149.53917  3.911665 3.98857254  0.30140243      0.81580866 \n",
       "23 0.94938161   129.17010 77.907751 4.00984293  0.39423793      0.08744255 \n",
       "24 0.60850101   426.19315 11.032080 3.44727788  0.22759002      0.97908562 \n",
       "25 0.03283095   435.86283  9.179517 4.74314211  0.98971872      0.03473545 \n",
       "26 0.90598062   499.15278 15.687057 2.03653188  0.88119443      0.02846176 \n",
       "27 0.88081637   347.04184 31.592234 0.68327497  0.05928076      0.57435605 \n",
       "28 0.73852995    26.99856 23.024449 4.87827067  0.03138657      0.76840357 \n",
       "29 0.21712107   391.46709 26.035756 0.09091459  0.16354992      0.11234408 \n",
       "30 0.30147697   248.26575 98.158734 2.60679969  0.85002898      0.72525885 \n",
       "   n_init_believers learning_rate con-threshold expertise_influence output  \n",
       "1  57.8611225       0.54632148    0.32160454    0.541202792         44.91015\n",
       "2  76.1547683       0.47592333    0.46328014    0.524350752         43.35827\n",
       "3  43.1984348       0.50098188    0.38406676    0.475940387         46.13721\n",
       "4  34.5217967       0.23294625    0.70413467    0.239238373         46.87023\n",
       "5  16.2605515       0.77409926    0.60234462    0.610609240         48.47599\n",
       "6  77.0902000       0.60907955    0.67106358    0.441360433         43.27983\n",
       "7  96.6376782       0.96163352    0.80923608    0.887869010         41.82674\n",
       "8  90.6509697       0.24464914    0.13135634    0.795441728         42.26746\n",
       "9  31.4393862       0.44938462    0.48563582    0.428534337         47.14763\n",
       "10 12.5805322       0.34182141    0.54708903    0.205643966         51.20797\n",
       "11 67.9886207       0.68151506    0.28799508    0.320194715         44.07850\n",
       "12 21.3105169       0.19295557    0.52876480    0.730052653         48.02256\n",
       "13 19.1455078       0.85751843    0.14812456    0.733731810         59.69117\n",
       "14 44.2274257       0.03241952    0.25700116    0.967152144         46.02098\n",
       "15 64.7271505       0.87168179    0.04355859    0.190592471         44.30201\n",
       "16 47.3906214       0.31178857    0.74889903    0.841312388         45.74135\n",
       "17 85.1104706       0.58659189    0.59300508    0.672293694         42.86651\n",
       "18 72.3019701       0.97725222    0.79485901    0.816627014         43.72011\n",
       "19 25.3765003       0.11726508    0.02038889    0.153329309         47.68389\n",
       "20 50.0465866       0.93145884    0.64156264    0.001923862         45.47320\n",
       "21 82.7361302       0.08154482    0.98428638    0.091779928         42.86634\n",
       "22  3.5105931       0.82436122    0.21835248    0.920309543         55.39434\n",
       "23  0.5524448       0.41810701    0.19107811    0.296165683          0.00000\n",
       "24 99.7437568       0.64141421    0.93629280    0.592873474         41.62802\n",
       "25 55.0380307       0.38222864    0.88022648    0.109845465         45.07540\n",
       "26 37.1724002       0.72339159    0.34528693    0.939470711         46.58785\n",
       "27 88.5115510       0.13893367    0.07742080    0.049369550         42.43369\n",
       "28 27.4412562       0.04733936    0.83423326    0.635451322         54.81774\n",
       "29  9.4471089       0.29020407    0.92921069    0.352597127         49.16110\n",
       "30 61.7155555       0.75383667    0.40803055    0.387801080         44.58299\n",
       "   seed\n",
       "1  0   \n",
       "2  0   \n",
       "3  0   \n",
       "4  0   \n",
       "5  0   \n",
       "6  0   \n",
       "7  0   \n",
       "8  0   \n",
       "9  0   \n",
       "10 0   \n",
       "11 0   \n",
       "12 0   \n",
       "13 0   \n",
       "14 0   \n",
       "15 0   \n",
       "16 0   \n",
       "17 0   \n",
       "18 0   \n",
       "19 0   \n",
       "20 0   \n",
       "21 0   \n",
       "22 0   \n",
       "23 0   \n",
       "24 0   \n",
       "25 0   \n",
       "26 0   \n",
       "27 0   \n",
       "28 0   \n",
       "29 0   \n",
       "30 0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_shot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8195cd",
   "metadata": {},
   "source": [
    "### One-shot Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4b3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.type = \"oneshot\"\n",
    "\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "iter = 1\n",
    "for (i in seed.focus) {\n",
    "    \n",
    "    training_set = copy(one_shot_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "    for (r in metarep) {\n",
    "        set.seed(i + r)\n",
    "        run_log_entry()\n",
    "\n",
    "        model_Sub <- randomForest(x = training_set[, -c(\"output\")], y = training_set$output, importance = TRUE, ntree = ntree, mtry = mtry, nperm = nperm)      \n",
    "        model_Sub.path = paste0(outputs.path,models.folder,\"model_\",sample.type,\"_seed_\", i, \"_rep_\", r,\"_size_\",train_ins_oneshot,\".rds\")\n",
    "        saveRDS(model_Sub, model_Sub.path)\n",
    "        \n",
    "        # write errors \n",
    "        obb_err = obb_error_func(model_Sub)     \n",
    "        fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "            ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "        \n",
    "        write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "        write_importance.rf(i,r,iter,model_Sub,sample.type)\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc070",
   "metadata": {},
   "source": [
    "### Adaptive Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaa4840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = upload_training_set(model.type,seed.focus,train_ins_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2e7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ground-truth</th><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>p-h-given-c</th><th scope=col>n_init_believers</th><th scope=col>learning_rate</th><th scope=col>con-threshold</th><th scope=col>expertise_influence</th><th scope=col>output</th><th scope=col>seed</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.47601769 </td><td>309.80882  </td><td>83.549839  </td><td>2.68861870 </td><td>0.65168982 </td><td>0.63354368 </td><td>57.8611225 </td><td>0.54632148 </td><td>0.32160454 </td><td>0.541202792</td><td>44.91015   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.35672834 </td><td>317.30518  </td><td>63.106664  </td><td>2.22335191 </td><td>0.90763209 </td><td>0.56252703 </td><td>76.1547683 </td><td>0.47592333 </td><td>0.46328014 </td><td>0.524350752</td><td>43.35827   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.71638853 </td><td> 80.00353  </td><td>58.991850  </td><td>2.40930323 </td><td>0.68969464 </td><td>0.74683716 </td><td>43.1984348 </td><td>0.50098188 </td><td>0.38406676 </td><td>0.475940387</td><td>46.13721   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.38894489 </td><td>268.21950  </td><td>83.062600  </td><td>3.66248914 </td><td>0.62880975 </td><td>0.60311785 </td><td>34.5217967 </td><td>0.23294625 </td><td>0.70413467 </td><td>0.239238373</td><td>46.87023   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.50152214 </td><td>376.17234  </td><td>68.169001  </td><td>2.96416124 </td><td>0.50981017 </td><td>0.86337559 </td><td>16.2605515 </td><td>0.77409926 </td><td>0.60234462 </td><td>0.610609240</td><td>48.47599   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.40541209 </td><td>263.49302  </td><td>47.911606  </td><td>1.78630814 </td><td>0.26884695 </td><td>0.45373039 </td><td>77.0902000 </td><td>0.60907955 </td><td>0.67106358 </td><td>0.441360433</td><td>43.27983   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.44819153 </td><td> 86.80230  </td><td>95.770899  </td><td>3.07717965 </td><td>0.46432791 </td><td>0.26671911 </td><td>96.6376782 </td><td>0.96163352 </td><td>0.80923608 </td><td>0.887869010</td><td>41.82674   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.53651635 </td><td>214.02388  </td><td>63.358828  </td><td>3.30239302 </td><td>0.18843268 </td><td>0.47324912 </td><td>90.6509697 </td><td>0.24464914 </td><td>0.13135634 </td><td>0.795441728</td><td>42.26746   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.07952148 </td><td>217.82492  </td><td>33.549733  </td><td>0.98303740 </td><td>0.78358728 </td><td>0.14660758 </td><td>31.4393862 </td><td>0.44938462 </td><td>0.48563582 </td><td>0.428534337</td><td>47.14763   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.24501809 </td><td> 50.51225  </td><td>74.346076  </td><td>1.62021644 </td><td>0.43220176 </td><td>0.89463243 </td><td>12.5805322 </td><td>0.34182141 </td><td>0.54708903 </td><td>0.205643966</td><td>51.20797   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.63856199 </td><td>459.48145  </td><td>53.224632  </td><td>4.40608682 </td><td>0.47381428 </td><td>0.25799465 </td><td>67.9886207 </td><td>0.68151506 </td><td>0.28799508 </td><td>0.320194715</td><td>44.07850   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.80488346 </td><td>354.04219  </td><td>46.331153  </td><td>0.37218905 </td><td>0.83127037 </td><td>0.95397055 </td><td>21.3105169 </td><td>0.19295557 </td><td>0.52876480 </td><td>0.730052653</td><td>48.02256   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.57630990 </td><td> 15.65373  </td><td>90.351600  </td><td>1.18339279 </td><td>0.54256842 </td><td>0.31927096 </td><td>19.1455078 </td><td>0.85751843 </td><td>0.14812456 </td><td>0.733731810</td><td>59.69117   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.77041118 </td><td>171.30952  </td><td>36.678126  </td><td>4.22830276 </td><td>0.72660916 </td><td>0.50263203 </td><td>44.2274257 </td><td>0.03241952 </td><td>0.25700116 </td><td>0.967152144</td><td>46.02098   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.14513135 </td><td>296.79148  </td><td> 2.711361  </td><td>1.83963641 </td><td>0.36661554 </td><td>0.33612344 </td><td>64.7271505 </td><td>0.87168179 </td><td>0.04355859 </td><td>0.190592471</td><td>44.30201   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.29182177 </td><td>403.67980  </td><td>70.859862  </td><td>3.69643211 </td><td>0.26371119 </td><td>0.42379335 </td><td>47.3906214 </td><td>0.31178857 </td><td>0.74889903 </td><td>0.841312388</td><td>45.74135   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.10346714 </td><td> 43.94761  </td><td>26.782110  </td><td>4.66349926 </td><td>0.58452664 </td><td>0.18292822 </td><td>85.1104706 </td><td>0.58659189 </td><td>0.59300508 </td><td>0.672293694</td><td>42.86651   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.98174934 </td><td>198.10849  </td><td>55.243804  </td><td>0.64423474 </td><td>0.10223249 </td><td>0.20594726 </td><td>72.3019701 </td><td>0.97725222 </td><td>0.79485901 </td><td>0.816627014</td><td>43.72011   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.03950368 </td><td>158.08886  </td><td>86.922408  </td><td>1.07490666 </td><td>0.09893722 </td><td>0.39887169 </td><td>25.3765003 </td><td>0.11726508 </td><td>0.02038889 </td><td>0.153329309</td><td>47.68389   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.85188611 </td><td>476.08661  </td><td>17.821875  </td><td>0.18955886 </td><td>0.95177300 </td><td>0.91359260 </td><td>50.0465866 </td><td>0.93145884 </td><td>0.64156264 </td><td>0.001923862</td><td>45.47320   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.67260541 </td><td>105.51007  </td><td>41.871069  </td><td>1.40523149 </td><td>0.76527629 </td><td>0.67846781 </td><td>82.7361302 </td><td>0.08154482 </td><td>0.98428638 </td><td>0.091779928</td><td>42.86634   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.19755468 </td><td>149.53917  </td><td> 3.911665  </td><td>3.98857254 </td><td>0.30140243 </td><td>0.81580866 </td><td> 3.5105931 </td><td>0.82436122 </td><td>0.21835248 </td><td>0.920309543</td><td>55.39434   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.94938161 </td><td>129.17010  </td><td>77.907751  </td><td>4.00984293 </td><td>0.39423793 </td><td>0.08744255 </td><td> 0.5524448 </td><td>0.41810701 </td><td>0.19107811 </td><td>0.296165683</td><td> 0.00000   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.60850101 </td><td>426.19315  </td><td>11.032080  </td><td>3.44727788 </td><td>0.22759002 </td><td>0.97908562 </td><td>99.7437568 </td><td>0.64141421 </td><td>0.93629280 </td><td>0.592873474</td><td>41.62802   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.03283095 </td><td>435.86283  </td><td> 9.179517  </td><td>4.74314211 </td><td>0.98971872 </td><td>0.03473545 </td><td>55.0380307 </td><td>0.38222864 </td><td>0.88022648 </td><td>0.109845465</td><td>45.07540   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.90598062 </td><td>499.15278  </td><td>15.687057  </td><td>2.03653188 </td><td>0.88119443 </td><td>0.02846176 </td><td>37.1724002 </td><td>0.72339159 </td><td>0.34528693 </td><td>0.939470711</td><td>46.58785   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.88081637 </td><td>347.04184  </td><td>31.592234  </td><td>0.68327497 </td><td>0.05928076 </td><td>0.57435605 </td><td>88.5115510 </td><td>0.13893367 </td><td>0.07742080 </td><td>0.049369550</td><td>42.43369   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.73852995 </td><td> 26.99856  </td><td>23.024449  </td><td>4.87827067 </td><td>0.03138657 </td><td>0.76840357 </td><td>27.4412562 </td><td>0.04733936 </td><td>0.83423326 </td><td>0.635451322</td><td>54.81774   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.21712107 </td><td>391.46709  </td><td>26.035756  </td><td>0.09091459 </td><td>0.16354992 </td><td>0.11234408 </td><td> 9.4471089 </td><td>0.29020407 </td><td>0.92921069 </td><td>0.352597127</td><td>49.16110   </td><td>0          </td></tr>\n",
       "\t<tr><td>0.30147697 </td><td>248.26575  </td><td>98.158734  </td><td>2.60679969 </td><td>0.85002898 </td><td>0.72525885 </td><td>61.7155555 </td><td>0.75383667 </td><td>0.40803055 </td><td>0.387801080</td><td>44.58299   </td><td>0          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " ground-truth & max\\_links & evidence & sc-bel-prop & prop-likelihood & p-h-given-c & n\\_init\\_believers & learning\\_rate & con-threshold & expertise\\_influence & output & seed\\\\\n",
       "\\hline\n",
       "\t 0.47601769  & 309.80882   & 83.549839   & 2.68861870  & 0.65168982  & 0.63354368  & 57.8611225  & 0.54632148  & 0.32160454  & 0.541202792 & 44.91015    & 0          \\\\\n",
       "\t 0.35672834  & 317.30518   & 63.106664   & 2.22335191  & 0.90763209  & 0.56252703  & 76.1547683  & 0.47592333  & 0.46328014  & 0.524350752 & 43.35827    & 0          \\\\\n",
       "\t 0.71638853  &  80.00353   & 58.991850   & 2.40930323  & 0.68969464  & 0.74683716  & 43.1984348  & 0.50098188  & 0.38406676  & 0.475940387 & 46.13721    & 0          \\\\\n",
       "\t 0.38894489  & 268.21950   & 83.062600   & 3.66248914  & 0.62880975  & 0.60311785  & 34.5217967  & 0.23294625  & 0.70413467  & 0.239238373 & 46.87023    & 0          \\\\\n",
       "\t 0.50152214  & 376.17234   & 68.169001   & 2.96416124  & 0.50981017  & 0.86337559  & 16.2605515  & 0.77409926  & 0.60234462  & 0.610609240 & 48.47599    & 0          \\\\\n",
       "\t 0.40541209  & 263.49302   & 47.911606   & 1.78630814  & 0.26884695  & 0.45373039  & 77.0902000  & 0.60907955  & 0.67106358  & 0.441360433 & 43.27983    & 0          \\\\\n",
       "\t 0.44819153  &  86.80230   & 95.770899   & 3.07717965  & 0.46432791  & 0.26671911  & 96.6376782  & 0.96163352  & 0.80923608  & 0.887869010 & 41.82674    & 0          \\\\\n",
       "\t 0.53651635  & 214.02388   & 63.358828   & 3.30239302  & 0.18843268  & 0.47324912  & 90.6509697  & 0.24464914  & 0.13135634  & 0.795441728 & 42.26746    & 0          \\\\\n",
       "\t 0.07952148  & 217.82492   & 33.549733   & 0.98303740  & 0.78358728  & 0.14660758  & 31.4393862  & 0.44938462  & 0.48563582  & 0.428534337 & 47.14763    & 0          \\\\\n",
       "\t 0.24501809  &  50.51225   & 74.346076   & 1.62021644  & 0.43220176  & 0.89463243  & 12.5805322  & 0.34182141  & 0.54708903  & 0.205643966 & 51.20797    & 0          \\\\\n",
       "\t 0.63856199  & 459.48145   & 53.224632   & 4.40608682  & 0.47381428  & 0.25799465  & 67.9886207  & 0.68151506  & 0.28799508  & 0.320194715 & 44.07850    & 0          \\\\\n",
       "\t 0.80488346  & 354.04219   & 46.331153   & 0.37218905  & 0.83127037  & 0.95397055  & 21.3105169  & 0.19295557  & 0.52876480  & 0.730052653 & 48.02256    & 0          \\\\\n",
       "\t 0.57630990  &  15.65373   & 90.351600   & 1.18339279  & 0.54256842  & 0.31927096  & 19.1455078  & 0.85751843  & 0.14812456  & 0.733731810 & 59.69117    & 0          \\\\\n",
       "\t 0.77041118  & 171.30952   & 36.678126   & 4.22830276  & 0.72660916  & 0.50263203  & 44.2274257  & 0.03241952  & 0.25700116  & 0.967152144 & 46.02098    & 0          \\\\\n",
       "\t 0.14513135  & 296.79148   &  2.711361   & 1.83963641  & 0.36661554  & 0.33612344  & 64.7271505  & 0.87168179  & 0.04355859  & 0.190592471 & 44.30201    & 0          \\\\\n",
       "\t 0.29182177  & 403.67980   & 70.859862   & 3.69643211  & 0.26371119  & 0.42379335  & 47.3906214  & 0.31178857  & 0.74889903  & 0.841312388 & 45.74135    & 0          \\\\\n",
       "\t 0.10346714  &  43.94761   & 26.782110   & 4.66349926  & 0.58452664  & 0.18292822  & 85.1104706  & 0.58659189  & 0.59300508  & 0.672293694 & 42.86651    & 0          \\\\\n",
       "\t 0.98174934  & 198.10849   & 55.243804   & 0.64423474  & 0.10223249  & 0.20594726  & 72.3019701  & 0.97725222  & 0.79485901  & 0.816627014 & 43.72011    & 0          \\\\\n",
       "\t 0.03950368  & 158.08886   & 86.922408   & 1.07490666  & 0.09893722  & 0.39887169  & 25.3765003  & 0.11726508  & 0.02038889  & 0.153329309 & 47.68389    & 0          \\\\\n",
       "\t 0.85188611  & 476.08661   & 17.821875   & 0.18955886  & 0.95177300  & 0.91359260  & 50.0465866  & 0.93145884  & 0.64156264  & 0.001923862 & 45.47320    & 0          \\\\\n",
       "\t 0.67260541  & 105.51007   & 41.871069   & 1.40523149  & 0.76527629  & 0.67846781  & 82.7361302  & 0.08154482  & 0.98428638  & 0.091779928 & 42.86634    & 0          \\\\\n",
       "\t 0.19755468  & 149.53917   &  3.911665   & 3.98857254  & 0.30140243  & 0.81580866  &  3.5105931  & 0.82436122  & 0.21835248  & 0.920309543 & 55.39434    & 0          \\\\\n",
       "\t 0.94938161  & 129.17010   & 77.907751   & 4.00984293  & 0.39423793  & 0.08744255  &  0.5524448  & 0.41810701  & 0.19107811  & 0.296165683 &  0.00000    & 0          \\\\\n",
       "\t 0.60850101  & 426.19315   & 11.032080   & 3.44727788  & 0.22759002  & 0.97908562  & 99.7437568  & 0.64141421  & 0.93629280  & 0.592873474 & 41.62802    & 0          \\\\\n",
       "\t 0.03283095  & 435.86283   &  9.179517   & 4.74314211  & 0.98971872  & 0.03473545  & 55.0380307  & 0.38222864  & 0.88022648  & 0.109845465 & 45.07540    & 0          \\\\\n",
       "\t 0.90598062  & 499.15278   & 15.687057   & 2.03653188  & 0.88119443  & 0.02846176  & 37.1724002  & 0.72339159  & 0.34528693  & 0.939470711 & 46.58785    & 0          \\\\\n",
       "\t 0.88081637  & 347.04184   & 31.592234   & 0.68327497  & 0.05928076  & 0.57435605  & 88.5115510  & 0.13893367  & 0.07742080  & 0.049369550 & 42.43369    & 0          \\\\\n",
       "\t 0.73852995  &  26.99856   & 23.024449   & 4.87827067  & 0.03138657  & 0.76840357  & 27.4412562  & 0.04733936  & 0.83423326  & 0.635451322 & 54.81774    & 0          \\\\\n",
       "\t 0.21712107  & 391.46709   & 26.035756   & 0.09091459  & 0.16354992  & 0.11234408  &  9.4471089  & 0.29020407  & 0.92921069  & 0.352597127 & 49.16110    & 0          \\\\\n",
       "\t 0.30147697  & 248.26575   & 98.158734   & 2.60679969  & 0.85002898  & 0.72525885  & 61.7155555  & 0.75383667  & 0.40803055  & 0.387801080 & 44.58299    & 0          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| ground-truth | max_links | evidence | sc-bel-prop | prop-likelihood | p-h-given-c | n_init_believers | learning_rate | con-threshold | expertise_influence | output | seed |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.47601769  | 309.80882   | 83.549839   | 2.68861870  | 0.65168982  | 0.63354368  | 57.8611225  | 0.54632148  | 0.32160454  | 0.541202792 | 44.91015    | 0           |\n",
       "| 0.35672834  | 317.30518   | 63.106664   | 2.22335191  | 0.90763209  | 0.56252703  | 76.1547683  | 0.47592333  | 0.46328014  | 0.524350752 | 43.35827    | 0           |\n",
       "| 0.71638853  |  80.00353   | 58.991850   | 2.40930323  | 0.68969464  | 0.74683716  | 43.1984348  | 0.50098188  | 0.38406676  | 0.475940387 | 46.13721    | 0           |\n",
       "| 0.38894489  | 268.21950   | 83.062600   | 3.66248914  | 0.62880975  | 0.60311785  | 34.5217967  | 0.23294625  | 0.70413467  | 0.239238373 | 46.87023    | 0           |\n",
       "| 0.50152214  | 376.17234   | 68.169001   | 2.96416124  | 0.50981017  | 0.86337559  | 16.2605515  | 0.77409926  | 0.60234462  | 0.610609240 | 48.47599    | 0           |\n",
       "| 0.40541209  | 263.49302   | 47.911606   | 1.78630814  | 0.26884695  | 0.45373039  | 77.0902000  | 0.60907955  | 0.67106358  | 0.441360433 | 43.27983    | 0           |\n",
       "| 0.44819153  |  86.80230   | 95.770899   | 3.07717965  | 0.46432791  | 0.26671911  | 96.6376782  | 0.96163352  | 0.80923608  | 0.887869010 | 41.82674    | 0           |\n",
       "| 0.53651635  | 214.02388   | 63.358828   | 3.30239302  | 0.18843268  | 0.47324912  | 90.6509697  | 0.24464914  | 0.13135634  | 0.795441728 | 42.26746    | 0           |\n",
       "| 0.07952148  | 217.82492   | 33.549733   | 0.98303740  | 0.78358728  | 0.14660758  | 31.4393862  | 0.44938462  | 0.48563582  | 0.428534337 | 47.14763    | 0           |\n",
       "| 0.24501809  |  50.51225   | 74.346076   | 1.62021644  | 0.43220176  | 0.89463243  | 12.5805322  | 0.34182141  | 0.54708903  | 0.205643966 | 51.20797    | 0           |\n",
       "| 0.63856199  | 459.48145   | 53.224632   | 4.40608682  | 0.47381428  | 0.25799465  | 67.9886207  | 0.68151506  | 0.28799508  | 0.320194715 | 44.07850    | 0           |\n",
       "| 0.80488346  | 354.04219   | 46.331153   | 0.37218905  | 0.83127037  | 0.95397055  | 21.3105169  | 0.19295557  | 0.52876480  | 0.730052653 | 48.02256    | 0           |\n",
       "| 0.57630990  |  15.65373   | 90.351600   | 1.18339279  | 0.54256842  | 0.31927096  | 19.1455078  | 0.85751843  | 0.14812456  | 0.733731810 | 59.69117    | 0           |\n",
       "| 0.77041118  | 171.30952   | 36.678126   | 4.22830276  | 0.72660916  | 0.50263203  | 44.2274257  | 0.03241952  | 0.25700116  | 0.967152144 | 46.02098    | 0           |\n",
       "| 0.14513135  | 296.79148   |  2.711361   | 1.83963641  | 0.36661554  | 0.33612344  | 64.7271505  | 0.87168179  | 0.04355859  | 0.190592471 | 44.30201    | 0           |\n",
       "| 0.29182177  | 403.67980   | 70.859862   | 3.69643211  | 0.26371119  | 0.42379335  | 47.3906214  | 0.31178857  | 0.74889903  | 0.841312388 | 45.74135    | 0           |\n",
       "| 0.10346714  |  43.94761   | 26.782110   | 4.66349926  | 0.58452664  | 0.18292822  | 85.1104706  | 0.58659189  | 0.59300508  | 0.672293694 | 42.86651    | 0           |\n",
       "| 0.98174934  | 198.10849   | 55.243804   | 0.64423474  | 0.10223249  | 0.20594726  | 72.3019701  | 0.97725222  | 0.79485901  | 0.816627014 | 43.72011    | 0           |\n",
       "| 0.03950368  | 158.08886   | 86.922408   | 1.07490666  | 0.09893722  | 0.39887169  | 25.3765003  | 0.11726508  | 0.02038889  | 0.153329309 | 47.68389    | 0           |\n",
       "| 0.85188611  | 476.08661   | 17.821875   | 0.18955886  | 0.95177300  | 0.91359260  | 50.0465866  | 0.93145884  | 0.64156264  | 0.001923862 | 45.47320    | 0           |\n",
       "| 0.67260541  | 105.51007   | 41.871069   | 1.40523149  | 0.76527629  | 0.67846781  | 82.7361302  | 0.08154482  | 0.98428638  | 0.091779928 | 42.86634    | 0           |\n",
       "| 0.19755468  | 149.53917   |  3.911665   | 3.98857254  | 0.30140243  | 0.81580866  |  3.5105931  | 0.82436122  | 0.21835248  | 0.920309543 | 55.39434    | 0           |\n",
       "| 0.94938161  | 129.17010   | 77.907751   | 4.00984293  | 0.39423793  | 0.08744255  |  0.5524448  | 0.41810701  | 0.19107811  | 0.296165683 |  0.00000    | 0           |\n",
       "| 0.60850101  | 426.19315   | 11.032080   | 3.44727788  | 0.22759002  | 0.97908562  | 99.7437568  | 0.64141421  | 0.93629280  | 0.592873474 | 41.62802    | 0           |\n",
       "| 0.03283095  | 435.86283   |  9.179517   | 4.74314211  | 0.98971872  | 0.03473545  | 55.0380307  | 0.38222864  | 0.88022648  | 0.109845465 | 45.07540    | 0           |\n",
       "| 0.90598062  | 499.15278   | 15.687057   | 2.03653188  | 0.88119443  | 0.02846176  | 37.1724002  | 0.72339159  | 0.34528693  | 0.939470711 | 46.58785    | 0           |\n",
       "| 0.88081637  | 347.04184   | 31.592234   | 0.68327497  | 0.05928076  | 0.57435605  | 88.5115510  | 0.13893367  | 0.07742080  | 0.049369550 | 42.43369    | 0           |\n",
       "| 0.73852995  |  26.99856   | 23.024449   | 4.87827067  | 0.03138657  | 0.76840357  | 27.4412562  | 0.04733936  | 0.83423326  | 0.635451322 | 54.81774    | 0           |\n",
       "| 0.21712107  | 391.46709   | 26.035756   | 0.09091459  | 0.16354992  | 0.11234408  |  9.4471089  | 0.29020407  | 0.92921069  | 0.352597127 | 49.16110    | 0           |\n",
       "| 0.30147697  | 248.26575   | 98.158734   | 2.60679969  | 0.85002898  | 0.72525885  | 61.7155555  | 0.75383667  | 0.40803055  | 0.387801080 | 44.58299    | 0           |\n",
       "\n"
      ],
      "text/plain": [
       "   ground-truth max_links evidence  sc-bel-prop prop-likelihood p-h-given-c\n",
       "1  0.47601769   309.80882 83.549839 2.68861870  0.65168982      0.63354368 \n",
       "2  0.35672834   317.30518 63.106664 2.22335191  0.90763209      0.56252703 \n",
       "3  0.71638853    80.00353 58.991850 2.40930323  0.68969464      0.74683716 \n",
       "4  0.38894489   268.21950 83.062600 3.66248914  0.62880975      0.60311785 \n",
       "5  0.50152214   376.17234 68.169001 2.96416124  0.50981017      0.86337559 \n",
       "6  0.40541209   263.49302 47.911606 1.78630814  0.26884695      0.45373039 \n",
       "7  0.44819153    86.80230 95.770899 3.07717965  0.46432791      0.26671911 \n",
       "8  0.53651635   214.02388 63.358828 3.30239302  0.18843268      0.47324912 \n",
       "9  0.07952148   217.82492 33.549733 0.98303740  0.78358728      0.14660758 \n",
       "10 0.24501809    50.51225 74.346076 1.62021644  0.43220176      0.89463243 \n",
       "11 0.63856199   459.48145 53.224632 4.40608682  0.47381428      0.25799465 \n",
       "12 0.80488346   354.04219 46.331153 0.37218905  0.83127037      0.95397055 \n",
       "13 0.57630990    15.65373 90.351600 1.18339279  0.54256842      0.31927096 \n",
       "14 0.77041118   171.30952 36.678126 4.22830276  0.72660916      0.50263203 \n",
       "15 0.14513135   296.79148  2.711361 1.83963641  0.36661554      0.33612344 \n",
       "16 0.29182177   403.67980 70.859862 3.69643211  0.26371119      0.42379335 \n",
       "17 0.10346714    43.94761 26.782110 4.66349926  0.58452664      0.18292822 \n",
       "18 0.98174934   198.10849 55.243804 0.64423474  0.10223249      0.20594726 \n",
       "19 0.03950368   158.08886 86.922408 1.07490666  0.09893722      0.39887169 \n",
       "20 0.85188611   476.08661 17.821875 0.18955886  0.95177300      0.91359260 \n",
       "21 0.67260541   105.51007 41.871069 1.40523149  0.76527629      0.67846781 \n",
       "22 0.19755468   149.53917  3.911665 3.98857254  0.30140243      0.81580866 \n",
       "23 0.94938161   129.17010 77.907751 4.00984293  0.39423793      0.08744255 \n",
       "24 0.60850101   426.19315 11.032080 3.44727788  0.22759002      0.97908562 \n",
       "25 0.03283095   435.86283  9.179517 4.74314211  0.98971872      0.03473545 \n",
       "26 0.90598062   499.15278 15.687057 2.03653188  0.88119443      0.02846176 \n",
       "27 0.88081637   347.04184 31.592234 0.68327497  0.05928076      0.57435605 \n",
       "28 0.73852995    26.99856 23.024449 4.87827067  0.03138657      0.76840357 \n",
       "29 0.21712107   391.46709 26.035756 0.09091459  0.16354992      0.11234408 \n",
       "30 0.30147697   248.26575 98.158734 2.60679969  0.85002898      0.72525885 \n",
       "   n_init_believers learning_rate con-threshold expertise_influence output  \n",
       "1  57.8611225       0.54632148    0.32160454    0.541202792         44.91015\n",
       "2  76.1547683       0.47592333    0.46328014    0.524350752         43.35827\n",
       "3  43.1984348       0.50098188    0.38406676    0.475940387         46.13721\n",
       "4  34.5217967       0.23294625    0.70413467    0.239238373         46.87023\n",
       "5  16.2605515       0.77409926    0.60234462    0.610609240         48.47599\n",
       "6  77.0902000       0.60907955    0.67106358    0.441360433         43.27983\n",
       "7  96.6376782       0.96163352    0.80923608    0.887869010         41.82674\n",
       "8  90.6509697       0.24464914    0.13135634    0.795441728         42.26746\n",
       "9  31.4393862       0.44938462    0.48563582    0.428534337         47.14763\n",
       "10 12.5805322       0.34182141    0.54708903    0.205643966         51.20797\n",
       "11 67.9886207       0.68151506    0.28799508    0.320194715         44.07850\n",
       "12 21.3105169       0.19295557    0.52876480    0.730052653         48.02256\n",
       "13 19.1455078       0.85751843    0.14812456    0.733731810         59.69117\n",
       "14 44.2274257       0.03241952    0.25700116    0.967152144         46.02098\n",
       "15 64.7271505       0.87168179    0.04355859    0.190592471         44.30201\n",
       "16 47.3906214       0.31178857    0.74889903    0.841312388         45.74135\n",
       "17 85.1104706       0.58659189    0.59300508    0.672293694         42.86651\n",
       "18 72.3019701       0.97725222    0.79485901    0.816627014         43.72011\n",
       "19 25.3765003       0.11726508    0.02038889    0.153329309         47.68389\n",
       "20 50.0465866       0.93145884    0.64156264    0.001923862         45.47320\n",
       "21 82.7361302       0.08154482    0.98428638    0.091779928         42.86634\n",
       "22  3.5105931       0.82436122    0.21835248    0.920309543         55.39434\n",
       "23  0.5524448       0.41810701    0.19107811    0.296165683          0.00000\n",
       "24 99.7437568       0.64141421    0.93629280    0.592873474         41.62802\n",
       "25 55.0380307       0.38222864    0.88022648    0.109845465         45.07540\n",
       "26 37.1724002       0.72339159    0.34528693    0.939470711         46.58785\n",
       "27 88.5115510       0.13893367    0.07742080    0.049369550         42.43369\n",
       "28 27.4412562       0.04733936    0.83423326    0.635451322         54.81774\n",
       "29  9.4471089       0.29020407    0.92921069    0.352597127         49.16110\n",
       "30 61.7155555       0.75383667    0.40803055    0.387801080         44.58299\n",
       "   seed\n",
       "1  0   \n",
       "2  0   \n",
       "3  0   \n",
       "4  0   \n",
       "5  0   \n",
       "6  0   \n",
       "7  0   \n",
       "8  0   \n",
       "9  0   \n",
       "10 0   \n",
       "11 0   \n",
       "12 0   \n",
       "13 0   \n",
       "14 0   \n",
       "15 0   \n",
       "16 0   \n",
       "17 0   \n",
       "18 0   \n",
       "19 0   \n",
       "20 0   \n",
       "21 0   \n",
       "22 0   \n",
       "23 0   \n",
       "24 0   \n",
       "25 0   \n",
       "26 0   \n",
       "27 0   \n",
       "28 0   \n",
       "29 0   \n",
       "30 0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptive_initial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca5fe9",
   "metadata": {},
   "source": [
    "### Random Sampling Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "431dc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"seed : 0  Random Sampling section start time : 2021-11-14 22:52:09\"\n",
      "[1] \"seed : 0   rep : 1  Random Sampling section start time : 2021-11-14 22:52:09\"\n",
      "[1] 1\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 22:52:09\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 22:56:03\"\n",
      "[1] 2\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 22:56:03\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 22:59:12\"\n",
      "[1] 3\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 22:59:12\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:01:40\"\n",
      "[1] 4\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:01:40\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:04:20\"\n",
      "[1] 5\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:04:20\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:06:57\"\n",
      "[1] 6\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:06:58\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:10:00\"\n",
      "[1] 7\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:10:00\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:12:35\"\n",
      "[1] 8\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:12:35\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:15:23\"\n",
      "[1] 9\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:15:23\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:18:24\"\n",
      "[1] 10\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:18:24\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:21:28\"\n",
      "[1] 11\n",
      "[1] \"seed : 0   rep : 1  Random Sampling section end time : 2021-11-14 23:21:29\"\n",
      "[1] \"seed : 0  Random Sampling section end time : 2021-11-14 23:21:29\"\n"
     ]
    }
   ],
   "source": [
    "sample.type = \"Rd\"\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "for (i in seed.focus) {\n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section start time : \", Sys.time()))\n",
    "    \n",
    "    for (r in metarep) { #replications\n",
    "        set.seed(i + r) # set seed to control randomness\n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section start time : \", Sys.time()))\n",
    "        \n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])       \n",
    "        train_candidates_table = data.table()\n",
    "            \n",
    "        iter = 1\n",
    "        while(iter <= iteration_budget){\n",
    "            print(iter)\n",
    "            run_log_entry()\n",
    "            \n",
    "            trainx = training_set_Ad[, .SD, .SDcols = feature_names]\n",
    "            trainy = training_set_Ad$output\n",
    "            \n",
    "            # Train the model\n",
    "            model_Sub <- randomForest(x = trainx, y = trainy, importance = TRUE,ntree = ntree, mtry = mtry, nperm = nperm)\n",
    "            model_Sub.name = paste0(\"model_\",sample.type,\"_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "            assign(model_Sub.name, model_Sub)\n",
    "            model_Sub.path = paste0(outputs.path,models.folder, paste0(model_Sub.name,\"_size_\",train_ins_Ad, \".rds\"))  # to save the model\n",
    "                saveRDS(model_Sub, model_Sub.path)\n",
    "            \n",
    "            # write errors \n",
    "            obb_err = obb_error_func(model_Sub)     \n",
    "            fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "                   ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "            \n",
    "            write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "            write_importance.rf(i,r,iter,model_Sub,sample.type)#last one=sample_type\n",
    "\n",
    "                   \n",
    "            if (iter != iteration_budget) {\n",
    "                ## sample selection from unlabeled data select candidates\n",
    "                 \n",
    "                unlabeled_set <- refresh_sample_pool(i + r + iter)\n",
    "                train_candidates = random_sample_selection(selected_ins, unlabeled_set)\n",
    "                                \n",
    "                # run ABM to find outputs of train candidates\n",
    "                print(paste0(\"ABM train_candidate run start time : \", Sys.time()))\n",
    "                train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "                print(paste0(\"ABM train_candidate run end time : \", Sys.time()))\n",
    "                \n",
    "                \n",
    "                fwrite(data.table(train_candidates, \"iter\" = iter, \"seed\" = i, \"rep\" = r)\n",
    "                       ,paste0(outputs.path,sample.folder,model.type,\"_train_candidates_table_\",sample.type,\".csv\"),append = TRUE )               \n",
    " \n",
    "                # Add new data to train data\n",
    "                training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")],use.names = TRUE)\n",
    "            }\n",
    "            iter = iter + 1\n",
    "        }  \n",
    "        \n",
    "        fwrite(data.table(training_set_Ad, \"seed\" = i,\"rep\" = r),paste0(outputs.path,sample.folder,model.type,\"_FinalTrainData_\",sample.type,\".csv\") ,append = TRUE)\n",
    "    \n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section end time : \", Sys.time()))        \n",
    "    }  \n",
    "         \n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section end time : \", Sys.time()))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970a28b",
   "metadata": {},
   "source": [
    "### Adaptive Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d1f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = upload_training_set(model.type,seed.focus,train_ins_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de55b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"seed : 0  Adaptive Sampling with coefvar  section start time : 2021-11-14 23:21:29\"\n",
      "[1] \"seed : 0   rep : 1  Adaptive Sampling section start time : 2021-11-14 23:21:29\"\n",
      "[1] 1\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:21:29\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:25:57\"\n",
      "[1] 2\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:25:57\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:27:57\"\n",
      "[1] 3\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:27:57\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:30:01\"\n",
      "[1] 4\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:30:02\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:36:16\"\n",
      "[1] 5\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:36:16\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:39:52\"\n",
      "[1] 6\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:39:52\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:43:12\"\n",
      "[1] 7\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:43:12\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:51:17\"\n",
      "[1] 8\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:51:17\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:54:39\"\n",
      "[1] 9\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:54:39\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-14 23:57:18\"\n",
      "[1] 10\n",
      "[1] \"ABM train_candidate run start time : 2021-11-14 23:57:18\"\n",
      "[1] \"ABM train_candidate run end time : 2021-11-15 00:01:18\"\n",
      "[1] 11\n",
      "[1] \"seed : 0   rep : 1  Adaptive Sampling section end time : 2021-11-15 00:01:18\"\n",
      "[1] \"seed : 0  Adaptive Sampling section end time : 2021-11-15 00:01:18\"\n"
     ]
    }
   ],
   "source": [
    "sample.type = paste0(\"Ad_\",selection_metric)\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "for(i in seed.focus){\n",
    "\n",
    "    print(paste0(\"seed : \",i,\"  Adaptive Sampling with \",selection_metric,\"  section start time : \",Sys.time()))\n",
    "    \n",
    "    for (r in metarep){ #replications\n",
    "        set.seed(i + r)\n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Adaptive Sampling section start time : \", Sys.time()))\n",
    "            \n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "        train_candidates_table = data.table()\n",
    "\n",
    "        iter = 1\n",
    "        while(iter <= iteration_budget){   \n",
    "            print(iter)\n",
    "            run_log_entry() \n",
    "        \n",
    "            trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "            trainy = training_set_Ad$output\n",
    "        \n",
    "            # Train the model\n",
    "            model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry,nperm = nperm)\n",
    "            model_Sub.name = paste0(\"model_\",sample.type,\"_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "            model_Sub.path = paste0(outputs.path,models.folder, paste0(model_Sub.name,\"_size_\",train_ins_Ad, \".rds\"))  # to save the model\n",
    "            saveRDS(model_Sub, model_Sub.path)\n",
    "        \n",
    "            # write errors \n",
    "            obb_err = obb_error_func(model_Sub)     \n",
    "            fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "                   ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "        \n",
    "            write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "            write_importance.rf(i,r,iter,model_Sub,sample.type)#last one=sample_type\n",
    "        \n",
    "            if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.    \n",
    "                ## sample selection from unlabeled data select candidates\n",
    "                unlabeled_set <- refresh_sample_pool(i + r + iter)\n",
    "            \n",
    "                train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub,selection_metric)\n",
    "            \n",
    "                # run ABM to find outputs of train candidates\n",
    "                print(paste0(\"ABM train_candidate run start time : \",Sys.time()))\n",
    "                train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "                print(paste0(\"ABM train_candidate run end time : \",Sys.time()))\n",
    "            \n",
    "                fwrite(data.table(train_candidates, \"iter\" = iter, \"seed\" = i, \"rep\" = r)\n",
    "                       ,paste0(outputs.path,sample.folder,model.type,\"_train_candidates_table_\",sample.type,\".csv\"),append = TRUE )      \n",
    "\n",
    "                # add labeled candidates to the train data\n",
    "                training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")],use.names = TRUE)\n",
    "            }  \n",
    "        \n",
    "            iter = iter + 1\n",
    "        }\n",
    "        fwrite(data.table(training_set_Ad, \"seed\" = i,\"rep\" = r),paste0(outputs.path,sample.folder,model.type,\"_FinalTrainData_\",sample.type,\".csv\") ,append = TRUE)\n",
    "    \n",
    "        print(paste0(\"seed : \",i,\"   rep : \", r,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "    }\n",
    "        \n",
    "    print(paste0(\"seed : \",i,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21251c",
   "metadata": {},
   "source": [
    "## Quit NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77f744b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLQuit(nl.obj = nl.model)\n",
    "#NLQuit(all=TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
