{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcd2bb2",
   "metadata": {},
   "source": [
    "## Loading Packages & Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a234b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1     ✔ purrr   0.3.2\n",
      "✔ tibble  3.1.0     ✔ dplyr   1.0.5\n",
      "✔ tidyr   1.1.3     ✔ stringr 1.4.0\n",
      "✔ readr   1.3.1     ✔ forcats 0.4.0\n",
      "Warning message:\n",
      "“package ‘tibble’ was built under R version 3.6.3”Warning message:\n",
      "“package ‘tidyr’ was built under R version 3.6.3”Warning message:\n",
      "“package ‘dplyr’ was built under R version 3.6.3”── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::between()   masks data.table::between()\n",
      "✖ dplyr::filter()    masks stats::filter()\n",
      "✖ dplyr::first()     masks data.table::first()\n",
      "✖ dplyr::lag()       masks stats::lag()\n",
      "✖ dplyr::last()      masks data.table::last()\n",
      "✖ purrr::transpose() masks data.table::transpose()\n",
      "Loading required package: igraph\n",
      "\n",
      "Attaching package: ‘igraph’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    as_data_frame, groups, union\n",
      "\n",
      "The following objects are masked from ‘package:purrr’:\n",
      "\n",
      "    compose, simplify\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    crossing\n",
      "\n",
      "The following object is masked from ‘package:tibble’:\n",
      "\n",
      "    as_data_frame\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    decompose, spectrum\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    union\n",
      "\n",
      "Warning message:\n",
      "“package ‘lhs’ was built under R version 3.6.3”"
     ]
    }
   ],
   "source": [
    "rm(list=ls())\n",
    "\n",
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(rJava)\n",
    "library(RNetLogo)\n",
    "library(lhs)\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f2ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘plotly’\n",
      "\n",
      "The following object is masked from ‘package:igraph’:\n",
      "\n",
      "    groups\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    last_plot\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    filter\n",
      "\n",
      "The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    layout\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n",
      "Loading required package: gridExtra\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "The following object is masked from ‘package:randomForest’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder.path = \"/Users/ecemnaz.yildiz/Documents/Personal/Thesis/\"\n",
    "source(paste0(folder.path,\"ThesisSetupCode.r\"))\n",
    "\n",
    "Is_Headless <- 1\n",
    "nl.model <- \"info_cascade_update_TDP_JPF_2020\"\n",
    "\n",
    "nl.path <- \"/Users/ecemnaz.yildiz/Documents/NetLogo 6.0.4/Java\"\n",
    "folder.path = \"/Users/ecemnaz.yildiz/Documents/Personal/Thesis/\"\n",
    "\n",
    "model.path <- paste0(folder.path, nl.model, \".nlogo\")\n",
    "\n",
    "if (Is_Headless == 0) {\n",
    "    NLStart(nl.path, gui = TRUE, nl.jarname = \"netlogo-6.0.4.jar\")\n",
    "    NLLoadModel(model.path)\n",
    "} else {\n",
    "    NLStart(nl.path, gui = FALSE, nl.jarname = \"netlogo-6.0.4.jar\", nl.obj = nl.model)\n",
    "    NLLoadModel(model.path, nl.obj = nl.model)\n",
    "}\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac15d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.type = \"info_cascade_update\"\n",
    "# the path of data folder\n",
    "data.path = paste0(folder.path,\"data/\")\n",
    "# the path for outputs to be record\n",
    "output.folder = paste0(\"outputs_AdS_cvar_TimeTrack_\",model.type,\"_\",Sys.Date())\n",
    "dir.create(file.path(folder.path, output.folder), showWarnings = FALSE)\n",
    "\n",
    "outputs.path = paste0(folder.path,output.folder,\"/\")\n",
    "\n",
    "# Read Me File to keep info about the output folder\n",
    "ReadMe = paste0(outputs.path,\"ReadMe_\",model.type,\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc78c8d",
   "metadata": {},
   "source": [
    "## Model Parameters & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2301c4b",
   "metadata": {},
   "source": [
    "### Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9abc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Parameters ####\n",
    "## Set model parameters Number of replications for each instance\n",
    "nofrep = 30\n",
    "\n",
    "feature_names = c(    \n",
    "    \"max_links\",\n",
    "    \"evidence\",\n",
    "    \"sc-bel-prop\",\n",
    "    \"prop-likelihood\",\n",
    "    \"n_init_believers\",\n",
    "    \"prior-mean\",\n",
    "    \"prior-sd\",\n",
    "    \"expertise_influence\") \n",
    "    \n",
    "feature_ranges = data.table(  feature   = feature_names\n",
    "                            , min_range = c(2, 0, 0, 0, 0, 0, 0, 0)\n",
    "                            , max_range = c(500, 100, 5, 1, 100, 1, 1, 1)\n",
    "                           )\n",
    "\n",
    "output_name = c(\"cl-prop-same\")\n",
    "\n",
    "# Number of input parameters of the agent-based model\n",
    "nofparams = length(feature_names)\n",
    "\n",
    "# set RF parameters\n",
    "ntree = 300\n",
    "mtry = 6\n",
    "nperm = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c5111",
   "metadata": {},
   "source": [
    "### Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9c2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User parameters ####\n",
    "error_type = \"RMSE\"  # MAPE, BIAS\n",
    "\n",
    "# choose the uncertainty measure\n",
    "selection_metric <- \"coefvar\"  #, 'range' \n",
    "sample.type = selection_metric\n",
    "\n",
    "# Number of iterations\n",
    "iteration_budget = 11\n",
    "metarep = c(1:1)\n",
    "\n",
    "# Number of instances\n",
    "unlabeled_ins = 30\n",
    "test_ins = c(30) #c(100,200,300,400)\n",
    "train_ins_oneshot = 30\n",
    "train_ins_Ad = 30\n",
    "\n",
    "# Set selection parameters\n",
    "selected_ins = 5  #nofinstancesWillbeSelected in each step\n",
    "\n",
    "# Set elimination parameters\n",
    "h <- 1  # number of variables eliminated in each step\n",
    "\n",
    "seed.focus = c(8)#,1,2,3,4,5,6,7,8,9,20)\n",
    "\n",
    "## to be used in log entries\n",
    "unlabeled.type = \"refresh\"\n",
    "\n",
    "log_entry.sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef606bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "8"
      ],
      "text/latex": [
       "8"
      ],
      "text/markdown": [
       "8"
      ],
      "text/plain": [
       "[1] 8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed.focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc953a",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5edb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test Sets ####\n",
    "test_set = data.table()\n",
    "\n",
    "for( t in test_ins){\n",
    "    test_set.name= paste0(data.path,\"test_set\",\"_\",model.type,\"_\",t,\".csv\")\n",
    "    test_set_Sub <- fread(test_set.name)  \n",
    "    \n",
    "    test_set = rbind(test_set, data.table(size = t, test_set_Sub))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2556d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>size</th><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>n_init_believers</th><th scope=col>prior-mean</th><th scope=col>prior-sd</th><th scope=col>expertise_influence</th><th scope=col>output</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>30         </td><td>153.52986  </td><td>17.822423  </td><td>3.5235470  </td><td>0.99942973 </td><td>21.415365  </td><td>0.31318558 </td><td>0.73333450 </td><td>0.388069845</td><td>48.10765   </td></tr>\n",
       "\t<tr><td>30         </td><td>238.41998  </td><td>25.829850  </td><td>1.6892055  </td><td>0.21765580 </td><td>66.338854  </td><td>0.69404958 </td><td>0.26549282 </td><td>0.529586065</td><td>44.14544   </td></tr>\n",
       "\t<tr><td>30         </td><td>496.77583  </td><td>80.553581  </td><td>0.5116790  </td><td>0.43575215 </td><td>92.243732  </td><td>0.20535621 </td><td>0.49550760 </td><td>0.549731941</td><td>42.16805   </td></tr>\n",
       "\t<tr><td>30         </td><td>261.28562  </td><td>16.483629  </td><td>3.3100315  </td><td>0.05897293 </td><td>15.308214  </td><td>0.89228352 </td><td>0.50357083 </td><td>0.239288878</td><td>51.75538   </td></tr>\n",
       "\t<tr><td>30         </td><td>421.92905  </td><td>97.740826  </td><td>3.6954132  </td><td>0.63071798 </td><td>43.870431  </td><td>0.58688183 </td><td>0.67234487 </td><td>0.782341848</td><td>46.06913   </td></tr>\n",
       "\t<tr><td>30         </td><td>362.21065  </td><td> 3.572477  </td><td>4.4936570  </td><td>0.88007035 </td><td>99.618287  </td><td>0.88094505 </td><td>0.43866759 </td><td>0.068812180</td><td>48.33022   </td></tr>\n",
       "\t<tr><td>30         </td><td>308.35369  </td><td>82.550522  </td><td>1.4803079  </td><td>0.11926421 </td><td>44.761687  </td><td>0.98306526 </td><td>0.41839219 </td><td>0.190624533</td><td>55.43060   </td></tr>\n",
       "\t<tr><td>30         </td><td>370.10732  </td><td> 4.114588  </td><td>2.6234796  </td><td>0.34156773 </td><td>81.519827  </td><td>0.32036658 </td><td>0.45222689 </td><td>0.043802018</td><td>44.59081   </td></tr>\n",
       "\t<tr><td>30         </td><td>209.78551  </td><td> 5.307582  </td><td>0.7697447  </td><td>0.85939482 </td><td>18.812904  </td><td>0.81556064 </td><td>0.66006938 </td><td>0.123153712</td><td>49.58548   </td></tr>\n",
       "\t<tr><td>30         </td><td>187.56724  </td><td>51.349104  </td><td>3.3438063  </td><td>0.61702112 </td><td>20.374490  </td><td>0.59864803 </td><td>0.20708671 </td><td>0.377884488</td><td>48.38556   </td></tr>\n",
       "\t<tr><td>30         </td><td>484.27418  </td><td>28.262651  </td><td>1.0478970  </td><td>0.15958112 </td><td>95.668522  </td><td>0.86270351 </td><td>0.47242841 </td><td>0.442012486</td><td>42.03604   </td></tr>\n",
       "\t<tr><td>30         </td><td>304.48924  </td><td>43.356757  </td><td>3.9949058  </td><td>0.14495172 </td><td>99.388221  </td><td>0.33065735 </td><td>0.56804264 </td><td>0.789083682</td><td>41.84859   </td></tr>\n",
       "\t<tr><td>30         </td><td>237.89184  </td><td>49.841497  </td><td>2.6132890  </td><td>0.70091386 </td><td>56.330844  </td><td>0.63978828 </td><td>0.36947281 </td><td>0.046631717</td><td>47.16762   </td></tr>\n",
       "\t<tr><td>30         </td><td>429.35362  </td><td> 1.188266  </td><td>0.4285442  </td><td>0.61385632 </td><td>77.818000  </td><td>0.38475187 </td><td>0.81616977 </td><td>0.501988798</td><td>43.27913   </td></tr>\n",
       "\t<tr><td>30         </td><td>218.80517  </td><td>24.948458  </td><td>4.1449879  </td><td>0.06800366 </td><td>98.235495  </td><td>0.26389852 </td><td>0.34533888 </td><td>0.092986771</td><td>47.57498   </td></tr>\n",
       "\t<tr><td>30         </td><td> 42.97026  </td><td>18.876938  </td><td>1.7029472  </td><td>0.33092071 </td><td> 9.313455  </td><td>0.14191936 </td><td>0.43623852 </td><td>0.247718807</td><td>62.47915   </td></tr>\n",
       "\t<tr><td>30         </td><td>211.55580  </td><td>14.694452  </td><td>0.6496062  </td><td>0.67052496 </td><td>38.437853  </td><td>0.74333837 </td><td>0.30813266 </td><td>0.288122695</td><td>49.54751   </td></tr>\n",
       "\t<tr><td>30         </td><td> 89.10651  </td><td>41.186112  </td><td>0.5236139  </td><td>0.23007210 </td><td>58.595874  </td><td>0.92618484 </td><td>0.13804640 </td><td>0.689026008</td><td>58.05833   </td></tr>\n",
       "\t<tr><td>30         </td><td>309.82657  </td><td>29.346832  </td><td>2.1014174  </td><td>0.92326174 </td><td>25.500563  </td><td>0.95336044 </td><td>0.21983866 </td><td>0.006391084</td><td>88.88736   </td></tr>\n",
       "\t<tr><td>30         </td><td>187.03468  </td><td>26.149174  </td><td>1.3483364  </td><td>0.03967006 </td><td>76.000086  </td><td>0.64663058 </td><td>0.77549871 </td><td>0.618781889</td><td>43.41377   </td></tr>\n",
       "\t<tr><td>30         </td><td>112.78570  </td><td>71.094397  </td><td>2.0481520  </td><td>0.47357670 </td><td>30.894127  </td><td>0.06318170 </td><td>0.04341185 </td><td>0.880838591</td><td>94.41411   </td></tr>\n",
       "\t<tr><td>30         </td><td>181.45572  </td><td>69.111081  </td><td>1.1718230  </td><td>0.15434942 </td><td>66.807605  </td><td>0.99393506 </td><td>0.42916233 </td><td>0.303813092</td><td>48.48793   </td></tr>\n",
       "\t<tr><td>30         </td><td>267.26543  </td><td>59.980319  </td><td>3.5591115  </td><td>0.94575559 </td><td>42.011349  </td><td>0.72318684 </td><td>0.21208180 </td><td>0.786316358</td><td>49.07646   </td></tr>\n",
       "\t<tr><td>30         </td><td>398.64183  </td><td>37.948013  </td><td>0.8382159  </td><td>0.97814190 </td><td>16.840550  </td><td>0.37937642 </td><td>0.94191537 </td><td>0.174570792</td><td>48.57160   </td></tr>\n",
       "\t<tr><td>30         </td><td>386.09132  </td><td>18.357766  </td><td>3.8649733  </td><td>0.19826890 </td><td>33.266757  </td><td>0.10747200 </td><td>0.07914372 </td><td>0.946020489</td><td>88.82066   </td></tr>\n",
       "\t<tr><td>30         </td><td>461.46066  </td><td>64.113033  </td><td>3.8208490  </td><td>0.80341982 </td><td>10.479836  </td><td>0.55243810 </td><td>0.70766570 </td><td>0.770229278</td><td>49.10857   </td></tr>\n",
       "\t<tr><td>30         </td><td>234.32666  </td><td>79.514139  </td><td>4.8622873  </td><td>0.85133509 </td><td>34.711122  </td><td>0.10198079 </td><td>0.70289951 </td><td>0.526290378</td><td>46.83409   </td></tr>\n",
       "\t<tr><td>30         </td><td>498.21381  </td><td>49.066915  </td><td>1.7609505  </td><td>0.31587438 </td><td>16.493433  </td><td>0.62701677 </td><td>0.79201951 </td><td>0.516147379</td><td>48.47286   </td></tr>\n",
       "\t<tr><td>30         </td><td>108.53198  </td><td> 5.934689  </td><td>4.2154127  </td><td>0.37974499 </td><td>63.554406  </td><td>0.11121458 </td><td>0.34542538 </td><td>0.848261417</td><td>49.61913   </td></tr>\n",
       "\t<tr><td>30         </td><td>108.95175  </td><td>14.634202  </td><td>0.2255047  </td><td>0.27621496 </td><td>87.765699  </td><td>0.01125575 </td><td>0.12508616 </td><td>0.758276936</td><td>70.63773   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " size & max\\_links & evidence & sc-bel-prop & prop-likelihood & n\\_init\\_believers & prior-mean & prior-sd & expertise\\_influence & output\\\\\n",
       "\\hline\n",
       "\t 30          & 153.52986   & 17.822423   & 3.5235470   & 0.99942973  & 21.415365   & 0.31318558  & 0.73333450  & 0.388069845 & 48.10765   \\\\\n",
       "\t 30          & 238.41998   & 25.829850   & 1.6892055   & 0.21765580  & 66.338854   & 0.69404958  & 0.26549282  & 0.529586065 & 44.14544   \\\\\n",
       "\t 30          & 496.77583   & 80.553581   & 0.5116790   & 0.43575215  & 92.243732   & 0.20535621  & 0.49550760  & 0.549731941 & 42.16805   \\\\\n",
       "\t 30          & 261.28562   & 16.483629   & 3.3100315   & 0.05897293  & 15.308214   & 0.89228352  & 0.50357083  & 0.239288878 & 51.75538   \\\\\n",
       "\t 30          & 421.92905   & 97.740826   & 3.6954132   & 0.63071798  & 43.870431   & 0.58688183  & 0.67234487  & 0.782341848 & 46.06913   \\\\\n",
       "\t 30          & 362.21065   &  3.572477   & 4.4936570   & 0.88007035  & 99.618287   & 0.88094505  & 0.43866759  & 0.068812180 & 48.33022   \\\\\n",
       "\t 30          & 308.35369   & 82.550522   & 1.4803079   & 0.11926421  & 44.761687   & 0.98306526  & 0.41839219  & 0.190624533 & 55.43060   \\\\\n",
       "\t 30          & 370.10732   &  4.114588   & 2.6234796   & 0.34156773  & 81.519827   & 0.32036658  & 0.45222689  & 0.043802018 & 44.59081   \\\\\n",
       "\t 30          & 209.78551   &  5.307582   & 0.7697447   & 0.85939482  & 18.812904   & 0.81556064  & 0.66006938  & 0.123153712 & 49.58548   \\\\\n",
       "\t 30          & 187.56724   & 51.349104   & 3.3438063   & 0.61702112  & 20.374490   & 0.59864803  & 0.20708671  & 0.377884488 & 48.38556   \\\\\n",
       "\t 30          & 484.27418   & 28.262651   & 1.0478970   & 0.15958112  & 95.668522   & 0.86270351  & 0.47242841  & 0.442012486 & 42.03604   \\\\\n",
       "\t 30          & 304.48924   & 43.356757   & 3.9949058   & 0.14495172  & 99.388221   & 0.33065735  & 0.56804264  & 0.789083682 & 41.84859   \\\\\n",
       "\t 30          & 237.89184   & 49.841497   & 2.6132890   & 0.70091386  & 56.330844   & 0.63978828  & 0.36947281  & 0.046631717 & 47.16762   \\\\\n",
       "\t 30          & 429.35362   &  1.188266   & 0.4285442   & 0.61385632  & 77.818000   & 0.38475187  & 0.81616977  & 0.501988798 & 43.27913   \\\\\n",
       "\t 30          & 218.80517   & 24.948458   & 4.1449879   & 0.06800366  & 98.235495   & 0.26389852  & 0.34533888  & 0.092986771 & 47.57498   \\\\\n",
       "\t 30          &  42.97026   & 18.876938   & 1.7029472   & 0.33092071  &  9.313455   & 0.14191936  & 0.43623852  & 0.247718807 & 62.47915   \\\\\n",
       "\t 30          & 211.55580   & 14.694452   & 0.6496062   & 0.67052496  & 38.437853   & 0.74333837  & 0.30813266  & 0.288122695 & 49.54751   \\\\\n",
       "\t 30          &  89.10651   & 41.186112   & 0.5236139   & 0.23007210  & 58.595874   & 0.92618484  & 0.13804640  & 0.689026008 & 58.05833   \\\\\n",
       "\t 30          & 309.82657   & 29.346832   & 2.1014174   & 0.92326174  & 25.500563   & 0.95336044  & 0.21983866  & 0.006391084 & 88.88736   \\\\\n",
       "\t 30          & 187.03468   & 26.149174   & 1.3483364   & 0.03967006  & 76.000086   & 0.64663058  & 0.77549871  & 0.618781889 & 43.41377   \\\\\n",
       "\t 30          & 112.78570   & 71.094397   & 2.0481520   & 0.47357670  & 30.894127   & 0.06318170  & 0.04341185  & 0.880838591 & 94.41411   \\\\\n",
       "\t 30          & 181.45572   & 69.111081   & 1.1718230   & 0.15434942  & 66.807605   & 0.99393506  & 0.42916233  & 0.303813092 & 48.48793   \\\\\n",
       "\t 30          & 267.26543   & 59.980319   & 3.5591115   & 0.94575559  & 42.011349   & 0.72318684  & 0.21208180  & 0.786316358 & 49.07646   \\\\\n",
       "\t 30          & 398.64183   & 37.948013   & 0.8382159   & 0.97814190  & 16.840550   & 0.37937642  & 0.94191537  & 0.174570792 & 48.57160   \\\\\n",
       "\t 30          & 386.09132   & 18.357766   & 3.8649733   & 0.19826890  & 33.266757   & 0.10747200  & 0.07914372  & 0.946020489 & 88.82066   \\\\\n",
       "\t 30          & 461.46066   & 64.113033   & 3.8208490   & 0.80341982  & 10.479836   & 0.55243810  & 0.70766570  & 0.770229278 & 49.10857   \\\\\n",
       "\t 30          & 234.32666   & 79.514139   & 4.8622873   & 0.85133509  & 34.711122   & 0.10198079  & 0.70289951  & 0.526290378 & 46.83409   \\\\\n",
       "\t 30          & 498.21381   & 49.066915   & 1.7609505   & 0.31587438  & 16.493433   & 0.62701677  & 0.79201951  & 0.516147379 & 48.47286   \\\\\n",
       "\t 30          & 108.53198   &  5.934689   & 4.2154127   & 0.37974499  & 63.554406   & 0.11121458  & 0.34542538  & 0.848261417 & 49.61913   \\\\\n",
       "\t 30          & 108.95175   & 14.634202   & 0.2255047   & 0.27621496  & 87.765699   & 0.01125575  & 0.12508616  & 0.758276936 & 70.63773   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| size | max_links | evidence | sc-bel-prop | prop-likelihood | n_init_believers | prior-mean | prior-sd | expertise_influence | output |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 30          | 153.52986   | 17.822423   | 3.5235470   | 0.99942973  | 21.415365   | 0.31318558  | 0.73333450  | 0.388069845 | 48.10765    |\n",
       "| 30          | 238.41998   | 25.829850   | 1.6892055   | 0.21765580  | 66.338854   | 0.69404958  | 0.26549282  | 0.529586065 | 44.14544    |\n",
       "| 30          | 496.77583   | 80.553581   | 0.5116790   | 0.43575215  | 92.243732   | 0.20535621  | 0.49550760  | 0.549731941 | 42.16805    |\n",
       "| 30          | 261.28562   | 16.483629   | 3.3100315   | 0.05897293  | 15.308214   | 0.89228352  | 0.50357083  | 0.239288878 | 51.75538    |\n",
       "| 30          | 421.92905   | 97.740826   | 3.6954132   | 0.63071798  | 43.870431   | 0.58688183  | 0.67234487  | 0.782341848 | 46.06913    |\n",
       "| 30          | 362.21065   |  3.572477   | 4.4936570   | 0.88007035  | 99.618287   | 0.88094505  | 0.43866759  | 0.068812180 | 48.33022    |\n",
       "| 30          | 308.35369   | 82.550522   | 1.4803079   | 0.11926421  | 44.761687   | 0.98306526  | 0.41839219  | 0.190624533 | 55.43060    |\n",
       "| 30          | 370.10732   |  4.114588   | 2.6234796   | 0.34156773  | 81.519827   | 0.32036658  | 0.45222689  | 0.043802018 | 44.59081    |\n",
       "| 30          | 209.78551   |  5.307582   | 0.7697447   | 0.85939482  | 18.812904   | 0.81556064  | 0.66006938  | 0.123153712 | 49.58548    |\n",
       "| 30          | 187.56724   | 51.349104   | 3.3438063   | 0.61702112  | 20.374490   | 0.59864803  | 0.20708671  | 0.377884488 | 48.38556    |\n",
       "| 30          | 484.27418   | 28.262651   | 1.0478970   | 0.15958112  | 95.668522   | 0.86270351  | 0.47242841  | 0.442012486 | 42.03604    |\n",
       "| 30          | 304.48924   | 43.356757   | 3.9949058   | 0.14495172  | 99.388221   | 0.33065735  | 0.56804264  | 0.789083682 | 41.84859    |\n",
       "| 30          | 237.89184   | 49.841497   | 2.6132890   | 0.70091386  | 56.330844   | 0.63978828  | 0.36947281  | 0.046631717 | 47.16762    |\n",
       "| 30          | 429.35362   |  1.188266   | 0.4285442   | 0.61385632  | 77.818000   | 0.38475187  | 0.81616977  | 0.501988798 | 43.27913    |\n",
       "| 30          | 218.80517   | 24.948458   | 4.1449879   | 0.06800366  | 98.235495   | 0.26389852  | 0.34533888  | 0.092986771 | 47.57498    |\n",
       "| 30          |  42.97026   | 18.876938   | 1.7029472   | 0.33092071  |  9.313455   | 0.14191936  | 0.43623852  | 0.247718807 | 62.47915    |\n",
       "| 30          | 211.55580   | 14.694452   | 0.6496062   | 0.67052496  | 38.437853   | 0.74333837  | 0.30813266  | 0.288122695 | 49.54751    |\n",
       "| 30          |  89.10651   | 41.186112   | 0.5236139   | 0.23007210  | 58.595874   | 0.92618484  | 0.13804640  | 0.689026008 | 58.05833    |\n",
       "| 30          | 309.82657   | 29.346832   | 2.1014174   | 0.92326174  | 25.500563   | 0.95336044  | 0.21983866  | 0.006391084 | 88.88736    |\n",
       "| 30          | 187.03468   | 26.149174   | 1.3483364   | 0.03967006  | 76.000086   | 0.64663058  | 0.77549871  | 0.618781889 | 43.41377    |\n",
       "| 30          | 112.78570   | 71.094397   | 2.0481520   | 0.47357670  | 30.894127   | 0.06318170  | 0.04341185  | 0.880838591 | 94.41411    |\n",
       "| 30          | 181.45572   | 69.111081   | 1.1718230   | 0.15434942  | 66.807605   | 0.99393506  | 0.42916233  | 0.303813092 | 48.48793    |\n",
       "| 30          | 267.26543   | 59.980319   | 3.5591115   | 0.94575559  | 42.011349   | 0.72318684  | 0.21208180  | 0.786316358 | 49.07646    |\n",
       "| 30          | 398.64183   | 37.948013   | 0.8382159   | 0.97814190  | 16.840550   | 0.37937642  | 0.94191537  | 0.174570792 | 48.57160    |\n",
       "| 30          | 386.09132   | 18.357766   | 3.8649733   | 0.19826890  | 33.266757   | 0.10747200  | 0.07914372  | 0.946020489 | 88.82066    |\n",
       "| 30          | 461.46066   | 64.113033   | 3.8208490   | 0.80341982  | 10.479836   | 0.55243810  | 0.70766570  | 0.770229278 | 49.10857    |\n",
       "| 30          | 234.32666   | 79.514139   | 4.8622873   | 0.85133509  | 34.711122   | 0.10198079  | 0.70289951  | 0.526290378 | 46.83409    |\n",
       "| 30          | 498.21381   | 49.066915   | 1.7609505   | 0.31587438  | 16.493433   | 0.62701677  | 0.79201951  | 0.516147379 | 48.47286    |\n",
       "| 30          | 108.53198   |  5.934689   | 4.2154127   | 0.37974499  | 63.554406   | 0.11121458  | 0.34542538  | 0.848261417 | 49.61913    |\n",
       "| 30          | 108.95175   | 14.634202   | 0.2255047   | 0.27621496  | 87.765699   | 0.01125575  | 0.12508616  | 0.758276936 | 70.63773    |\n",
       "\n"
      ],
      "text/plain": [
       "   size max_links evidence  sc-bel-prop prop-likelihood n_init_believers\n",
       "1  30   153.52986 17.822423 3.5235470   0.99942973      21.415365       \n",
       "2  30   238.41998 25.829850 1.6892055   0.21765580      66.338854       \n",
       "3  30   496.77583 80.553581 0.5116790   0.43575215      92.243732       \n",
       "4  30   261.28562 16.483629 3.3100315   0.05897293      15.308214       \n",
       "5  30   421.92905 97.740826 3.6954132   0.63071798      43.870431       \n",
       "6  30   362.21065  3.572477 4.4936570   0.88007035      99.618287       \n",
       "7  30   308.35369 82.550522 1.4803079   0.11926421      44.761687       \n",
       "8  30   370.10732  4.114588 2.6234796   0.34156773      81.519827       \n",
       "9  30   209.78551  5.307582 0.7697447   0.85939482      18.812904       \n",
       "10 30   187.56724 51.349104 3.3438063   0.61702112      20.374490       \n",
       "11 30   484.27418 28.262651 1.0478970   0.15958112      95.668522       \n",
       "12 30   304.48924 43.356757 3.9949058   0.14495172      99.388221       \n",
       "13 30   237.89184 49.841497 2.6132890   0.70091386      56.330844       \n",
       "14 30   429.35362  1.188266 0.4285442   0.61385632      77.818000       \n",
       "15 30   218.80517 24.948458 4.1449879   0.06800366      98.235495       \n",
       "16 30    42.97026 18.876938 1.7029472   0.33092071       9.313455       \n",
       "17 30   211.55580 14.694452 0.6496062   0.67052496      38.437853       \n",
       "18 30    89.10651 41.186112 0.5236139   0.23007210      58.595874       \n",
       "19 30   309.82657 29.346832 2.1014174   0.92326174      25.500563       \n",
       "20 30   187.03468 26.149174 1.3483364   0.03967006      76.000086       \n",
       "21 30   112.78570 71.094397 2.0481520   0.47357670      30.894127       \n",
       "22 30   181.45572 69.111081 1.1718230   0.15434942      66.807605       \n",
       "23 30   267.26543 59.980319 3.5591115   0.94575559      42.011349       \n",
       "24 30   398.64183 37.948013 0.8382159   0.97814190      16.840550       \n",
       "25 30   386.09132 18.357766 3.8649733   0.19826890      33.266757       \n",
       "26 30   461.46066 64.113033 3.8208490   0.80341982      10.479836       \n",
       "27 30   234.32666 79.514139 4.8622873   0.85133509      34.711122       \n",
       "28 30   498.21381 49.066915 1.7609505   0.31587438      16.493433       \n",
       "29 30   108.53198  5.934689 4.2154127   0.37974499      63.554406       \n",
       "30 30   108.95175 14.634202 0.2255047   0.27621496      87.765699       \n",
       "   prior-mean prior-sd   expertise_influence output  \n",
       "1  0.31318558 0.73333450 0.388069845         48.10765\n",
       "2  0.69404958 0.26549282 0.529586065         44.14544\n",
       "3  0.20535621 0.49550760 0.549731941         42.16805\n",
       "4  0.89228352 0.50357083 0.239288878         51.75538\n",
       "5  0.58688183 0.67234487 0.782341848         46.06913\n",
       "6  0.88094505 0.43866759 0.068812180         48.33022\n",
       "7  0.98306526 0.41839219 0.190624533         55.43060\n",
       "8  0.32036658 0.45222689 0.043802018         44.59081\n",
       "9  0.81556064 0.66006938 0.123153712         49.58548\n",
       "10 0.59864803 0.20708671 0.377884488         48.38556\n",
       "11 0.86270351 0.47242841 0.442012486         42.03604\n",
       "12 0.33065735 0.56804264 0.789083682         41.84859\n",
       "13 0.63978828 0.36947281 0.046631717         47.16762\n",
       "14 0.38475187 0.81616977 0.501988798         43.27913\n",
       "15 0.26389852 0.34533888 0.092986771         47.57498\n",
       "16 0.14191936 0.43623852 0.247718807         62.47915\n",
       "17 0.74333837 0.30813266 0.288122695         49.54751\n",
       "18 0.92618484 0.13804640 0.689026008         58.05833\n",
       "19 0.95336044 0.21983866 0.006391084         88.88736\n",
       "20 0.64663058 0.77549871 0.618781889         43.41377\n",
       "21 0.06318170 0.04341185 0.880838591         94.41411\n",
       "22 0.99393506 0.42916233 0.303813092         48.48793\n",
       "23 0.72318684 0.21208180 0.786316358         49.07646\n",
       "24 0.37937642 0.94191537 0.174570792         48.57160\n",
       "25 0.10747200 0.07914372 0.946020489         88.82066\n",
       "26 0.55243810 0.70766570 0.770229278         49.10857\n",
       "27 0.10198079 0.70289951 0.526290378         46.83409\n",
       "28 0.62701677 0.79201951 0.516147379         48.47286\n",
       "29 0.11121458 0.34542538 0.848261417         49.61913\n",
       "30 0.01125575 0.12508616 0.758276936         70.63773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf8b34",
   "metadata": {},
   "source": [
    "## One-Shot Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8ab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_data = upload_training_set(model.type,seed.focus,train_ins_oneshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59aa3b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>n_init_believers</th><th scope=col>prior-mean</th><th scope=col>prior-sd</th><th scope=col>expertise_influence</th><th scope=col>output</th><th scope=col>seed</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>379.030421</td><td>33.093892 </td><td>2.3776148 </td><td>0.45018803</td><td>81.489234 </td><td>0.25385494</td><td>0.55607425</td><td>0.44670360</td><td>42.97597  </td><td>8         </td></tr>\n",
       "\t<tr><td>354.720810</td><td>53.770374 </td><td>1.7306876 </td><td>0.39097777</td><td>51.108860 </td><td>0.30752116</td><td>0.74643066</td><td>0.49186844</td><td>45.43132  </td><td>8         </td></tr>\n",
       "\t<tr><td>150.663956</td><td>52.282630 </td><td>2.5397536 </td><td>0.35723122</td><td>49.568888 </td><td>0.46417893</td><td>0.20238519</td><td>0.59242957</td><td>45.53525  </td><td>8         </td></tr>\n",
       "\t<tr><td>310.303729</td><td>37.939215 </td><td>1.6481274 </td><td>0.79260720</td><td>39.472428 </td><td>0.41508232</td><td>0.49821941</td><td>0.54886696</td><td>46.41394  </td><td>8         </td></tr>\n",
       "\t<tr><td>246.681021</td><td>69.824069 </td><td>2.1237128 </td><td>0.49174655</td><td>79.052799 </td><td>0.66834203</td><td>0.87004269</td><td>0.41265428</td><td>43.12122  </td><td>8         </td></tr>\n",
       "\t<tr><td>284.665309</td><td> 2.769528 </td><td>4.5423103 </td><td>0.62618815</td><td>36.565307 </td><td>0.37020836</td><td>0.42145825</td><td>0.70496254</td><td>46.85059  </td><td>8         </td></tr>\n",
       "\t<tr><td> 68.156233</td><td>17.351079 </td><td>3.1303719 </td><td>0.69802720</td><td>46.491671 </td><td>0.35416725</td><td>0.72813793</td><td>0.35974346</td><td>45.94112  </td><td>8         </td></tr>\n",
       "\t<tr><td>395.927951</td><td>72.373656 </td><td>4.0555379 </td><td>0.25140501</td><td>54.704373 </td><td>0.74388255</td><td>0.07464179</td><td>0.76934667</td><td>45.96783  </td><td>8         </td></tr>\n",
       "\t<tr><td>265.703127</td><td>79.724977 </td><td>3.9051546 </td><td>0.71919613</td><td>60.588566 </td><td>0.21410364</td><td>0.32198382</td><td>0.32189261</td><td>47.28248  </td><td>8         </td></tr>\n",
       "\t<tr><td>223.475741</td><td>60.791212 </td><td>3.5232455 </td><td>0.99764996</td><td>11.128286 </td><td>0.93562664</td><td>0.33789561</td><td>0.36937890</td><td>53.15028  </td><td>8         </td></tr>\n",
       "\t<tr><td>421.236560</td><td>66.295107 </td><td>0.3364330 </td><td>0.86289680</td><td>69.176981 </td><td>0.58804706</td><td>0.27009668</td><td>0.95848643</td><td>44.62737  </td><td>8         </td></tr>\n",
       "\t<tr><td>130.072200</td><td>40.463308 </td><td>1.8951732 </td><td>0.23066323</td><td>92.404291 </td><td>0.79270186</td><td>0.18821791</td><td>0.26481386</td><td>52.10164  </td><td>8         </td></tr>\n",
       "\t<tr><td>270.502395</td><td>34.186131 </td><td>1.3370186 </td><td>0.33041472</td><td>93.966827 </td><td>0.29134681</td><td>0.63820563</td><td>0.82755395</td><td>42.25718  </td><td>8         </td></tr>\n",
       "\t<tr><td> 38.009260</td><td>88.114134 </td><td>0.1159223 </td><td>0.75591975</td><td>26.677360 </td><td>0.49086524</td><td>0.51166162</td><td>0.51026560</td><td>49.48517  </td><td>8         </td></tr>\n",
       "\t<tr><td>105.945047</td><td>13.406723 </td><td>2.8871286 </td><td>0.18235217</td><td>24.233537 </td><td>0.91804698</td><td>0.44660403</td><td>0.68047562</td><td>50.16785  </td><td>8         </td></tr>\n",
       "\t<tr><td>329.459638</td><td>74.665187 </td><td>4.7043965 </td><td>0.92781913</td><td>31.755795 </td><td>0.15836933</td><td>0.69424139</td><td>0.85872020</td><td>47.57652  </td><td>8         </td></tr>\n",
       "\t<tr><td>445.433377</td><td>20.163957 </td><td>2.8094550 </td><td>0.64871435</td><td>65.197916 </td><td>0.19703505</td><td>0.01651411</td><td>0.27272424</td><td>85.44612  </td><td>8         </td></tr>\n",
       "\t<tr><td>205.299918</td><td>82.925215 </td><td>4.2510232 </td><td>0.06885088</td><td>40.729035 </td><td>0.50699671</td><td>0.78870993</td><td>0.03180102</td><td>46.33829  </td><td>8         </td></tr>\n",
       "\t<tr><td>172.020981</td><td>90.914017 </td><td>0.7537905 </td><td>0.04963815</td><td>58.502033 </td><td>0.61020586</td><td>0.38904975</td><td>0.87583322</td><td>45.21431  </td><td>8         </td></tr>\n",
       "\t<tr><td>460.201816</td><td>96.436939 </td><td>3.2751746 </td><td>0.41358148</td><td>72.334957 </td><td>0.97636854</td><td>0.24401142</td><td>0.08920294</td><td>77.54047  </td><td>8         </td></tr>\n",
       "\t<tr><td>100.811706</td><td>29.419096 </td><td>0.5983277 </td><td>0.81937961</td><td>73.939981 </td><td>0.54614157</td><td>0.10760606</td><td>0.62401049</td><td>43.65235  </td><td>8         </td></tr>\n",
       "\t<tr><td>344.588980</td><td>56.761968 </td><td>1.2317070 </td><td>0.88367730</td><td>14.666690 </td><td>0.80958359</td><td>0.91395235</td><td>0.90586485</td><td>48.84483  </td><td>8         </td></tr>\n",
       "\t<tr><td>497.046259</td><td> 6.409169 </td><td>3.4342425 </td><td>0.10687218</td><td>17.011417 </td><td>0.11113822</td><td>0.85546841</td><td>0.64210018</td><td>48.49530  </td><td>8         </td></tr>\n",
       "\t<tr><td> 33.715980</td><td>96.878792 </td><td>3.6883370 </td><td>0.01331588</td><td> 9.841476 </td><td>0.06459131</td><td>0.57659928</td><td>0.98300466</td><td>62.21116  </td><td>8         </td></tr>\n",
       "\t<tr><td> 73.873047</td><td>12.010874 </td><td>4.3941673 </td><td>0.28659854</td><td>22.609673 </td><td>0.03020353</td><td>0.04830248</td><td>0.22913679</td><td>95.87331  </td><td>8         </td></tr>\n",
       "\t<tr><td>186.345115</td><td>85.015145 </td><td>0.9108377 </td><td>0.54842970</td><td>87.681003 </td><td>0.07166630</td><td>0.80328097</td><td>0.11883268</td><td>43.34818  </td><td>8         </td></tr>\n",
       "\t<tr><td>  7.535025</td><td>26.249094 </td><td>2.2641867 </td><td>0.94330607</td><td> 1.057592 </td><td>0.83857927</td><td>0.98857611</td><td>0.05042143</td><td>52.57788  </td><td>8         </td></tr>\n",
       "\t<tr><td>468.942075</td><td> 7.031200 </td><td>1.0194360 </td><td>0.14049176</td><td>98.277931 </td><td>0.89776848</td><td>0.96635918</td><td>0.14922773</td><td>41.99267  </td><td>8         </td></tr>\n",
       "\t<tr><td>409.844151</td><td>45.865906 </td><td>0.1690033 </td><td>0.58936029</td><td> 3.752769 </td><td>0.63345464</td><td>0.15235143</td><td>0.18114986</td><td>58.78656  </td><td>8         </td></tr>\n",
       "\t<tr><td>158.671688</td><td>49.715070 </td><td>4.9558383 </td><td>0.52870032</td><td>85.371960 </td><td>0.70624448</td><td>0.62319670</td><td>0.75789638</td><td>42.77706  </td><td>8         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " max\\_links & evidence & sc-bel-prop & prop-likelihood & n\\_init\\_believers & prior-mean & prior-sd & expertise\\_influence & output & seed\\\\\n",
       "\\hline\n",
       "\t 379.030421 & 33.093892  & 2.3776148  & 0.45018803 & 81.489234  & 0.25385494 & 0.55607425 & 0.44670360 & 42.97597   & 8         \\\\\n",
       "\t 354.720810 & 53.770374  & 1.7306876  & 0.39097777 & 51.108860  & 0.30752116 & 0.74643066 & 0.49186844 & 45.43132   & 8         \\\\\n",
       "\t 150.663956 & 52.282630  & 2.5397536  & 0.35723122 & 49.568888  & 0.46417893 & 0.20238519 & 0.59242957 & 45.53525   & 8         \\\\\n",
       "\t 310.303729 & 37.939215  & 1.6481274  & 0.79260720 & 39.472428  & 0.41508232 & 0.49821941 & 0.54886696 & 46.41394   & 8         \\\\\n",
       "\t 246.681021 & 69.824069  & 2.1237128  & 0.49174655 & 79.052799  & 0.66834203 & 0.87004269 & 0.41265428 & 43.12122   & 8         \\\\\n",
       "\t 284.665309 &  2.769528  & 4.5423103  & 0.62618815 & 36.565307  & 0.37020836 & 0.42145825 & 0.70496254 & 46.85059   & 8         \\\\\n",
       "\t  68.156233 & 17.351079  & 3.1303719  & 0.69802720 & 46.491671  & 0.35416725 & 0.72813793 & 0.35974346 & 45.94112   & 8         \\\\\n",
       "\t 395.927951 & 72.373656  & 4.0555379  & 0.25140501 & 54.704373  & 0.74388255 & 0.07464179 & 0.76934667 & 45.96783   & 8         \\\\\n",
       "\t 265.703127 & 79.724977  & 3.9051546  & 0.71919613 & 60.588566  & 0.21410364 & 0.32198382 & 0.32189261 & 47.28248   & 8         \\\\\n",
       "\t 223.475741 & 60.791212  & 3.5232455  & 0.99764996 & 11.128286  & 0.93562664 & 0.33789561 & 0.36937890 & 53.15028   & 8         \\\\\n",
       "\t 421.236560 & 66.295107  & 0.3364330  & 0.86289680 & 69.176981  & 0.58804706 & 0.27009668 & 0.95848643 & 44.62737   & 8         \\\\\n",
       "\t 130.072200 & 40.463308  & 1.8951732  & 0.23066323 & 92.404291  & 0.79270186 & 0.18821791 & 0.26481386 & 52.10164   & 8         \\\\\n",
       "\t 270.502395 & 34.186131  & 1.3370186  & 0.33041472 & 93.966827  & 0.29134681 & 0.63820563 & 0.82755395 & 42.25718   & 8         \\\\\n",
       "\t  38.009260 & 88.114134  & 0.1159223  & 0.75591975 & 26.677360  & 0.49086524 & 0.51166162 & 0.51026560 & 49.48517   & 8         \\\\\n",
       "\t 105.945047 & 13.406723  & 2.8871286  & 0.18235217 & 24.233537  & 0.91804698 & 0.44660403 & 0.68047562 & 50.16785   & 8         \\\\\n",
       "\t 329.459638 & 74.665187  & 4.7043965  & 0.92781913 & 31.755795  & 0.15836933 & 0.69424139 & 0.85872020 & 47.57652   & 8         \\\\\n",
       "\t 445.433377 & 20.163957  & 2.8094550  & 0.64871435 & 65.197916  & 0.19703505 & 0.01651411 & 0.27272424 & 85.44612   & 8         \\\\\n",
       "\t 205.299918 & 82.925215  & 4.2510232  & 0.06885088 & 40.729035  & 0.50699671 & 0.78870993 & 0.03180102 & 46.33829   & 8         \\\\\n",
       "\t 172.020981 & 90.914017  & 0.7537905  & 0.04963815 & 58.502033  & 0.61020586 & 0.38904975 & 0.87583322 & 45.21431   & 8         \\\\\n",
       "\t 460.201816 & 96.436939  & 3.2751746  & 0.41358148 & 72.334957  & 0.97636854 & 0.24401142 & 0.08920294 & 77.54047   & 8         \\\\\n",
       "\t 100.811706 & 29.419096  & 0.5983277  & 0.81937961 & 73.939981  & 0.54614157 & 0.10760606 & 0.62401049 & 43.65235   & 8         \\\\\n",
       "\t 344.588980 & 56.761968  & 1.2317070  & 0.88367730 & 14.666690  & 0.80958359 & 0.91395235 & 0.90586485 & 48.84483   & 8         \\\\\n",
       "\t 497.046259 &  6.409169  & 3.4342425  & 0.10687218 & 17.011417  & 0.11113822 & 0.85546841 & 0.64210018 & 48.49530   & 8         \\\\\n",
       "\t  33.715980 & 96.878792  & 3.6883370  & 0.01331588 &  9.841476  & 0.06459131 & 0.57659928 & 0.98300466 & 62.21116   & 8         \\\\\n",
       "\t  73.873047 & 12.010874  & 4.3941673  & 0.28659854 & 22.609673  & 0.03020353 & 0.04830248 & 0.22913679 & 95.87331   & 8         \\\\\n",
       "\t 186.345115 & 85.015145  & 0.9108377  & 0.54842970 & 87.681003  & 0.07166630 & 0.80328097 & 0.11883268 & 43.34818   & 8         \\\\\n",
       "\t   7.535025 & 26.249094  & 2.2641867  & 0.94330607 &  1.057592  & 0.83857927 & 0.98857611 & 0.05042143 & 52.57788   & 8         \\\\\n",
       "\t 468.942075 &  7.031200  & 1.0194360  & 0.14049176 & 98.277931  & 0.89776848 & 0.96635918 & 0.14922773 & 41.99267   & 8         \\\\\n",
       "\t 409.844151 & 45.865906  & 0.1690033  & 0.58936029 &  3.752769  & 0.63345464 & 0.15235143 & 0.18114986 & 58.78656   & 8         \\\\\n",
       "\t 158.671688 & 49.715070  & 4.9558383  & 0.52870032 & 85.371960  & 0.70624448 & 0.62319670 & 0.75789638 & 42.77706   & 8         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| max_links | evidence | sc-bel-prop | prop-likelihood | n_init_believers | prior-mean | prior-sd | expertise_influence | output | seed |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 379.030421 | 33.093892  | 2.3776148  | 0.45018803 | 81.489234  | 0.25385494 | 0.55607425 | 0.44670360 | 42.97597   | 8          |\n",
       "| 354.720810 | 53.770374  | 1.7306876  | 0.39097777 | 51.108860  | 0.30752116 | 0.74643066 | 0.49186844 | 45.43132   | 8          |\n",
       "| 150.663956 | 52.282630  | 2.5397536  | 0.35723122 | 49.568888  | 0.46417893 | 0.20238519 | 0.59242957 | 45.53525   | 8          |\n",
       "| 310.303729 | 37.939215  | 1.6481274  | 0.79260720 | 39.472428  | 0.41508232 | 0.49821941 | 0.54886696 | 46.41394   | 8          |\n",
       "| 246.681021 | 69.824069  | 2.1237128  | 0.49174655 | 79.052799  | 0.66834203 | 0.87004269 | 0.41265428 | 43.12122   | 8          |\n",
       "| 284.665309 |  2.769528  | 4.5423103  | 0.62618815 | 36.565307  | 0.37020836 | 0.42145825 | 0.70496254 | 46.85059   | 8          |\n",
       "|  68.156233 | 17.351079  | 3.1303719  | 0.69802720 | 46.491671  | 0.35416725 | 0.72813793 | 0.35974346 | 45.94112   | 8          |\n",
       "| 395.927951 | 72.373656  | 4.0555379  | 0.25140501 | 54.704373  | 0.74388255 | 0.07464179 | 0.76934667 | 45.96783   | 8          |\n",
       "| 265.703127 | 79.724977  | 3.9051546  | 0.71919613 | 60.588566  | 0.21410364 | 0.32198382 | 0.32189261 | 47.28248   | 8          |\n",
       "| 223.475741 | 60.791212  | 3.5232455  | 0.99764996 | 11.128286  | 0.93562664 | 0.33789561 | 0.36937890 | 53.15028   | 8          |\n",
       "| 421.236560 | 66.295107  | 0.3364330  | 0.86289680 | 69.176981  | 0.58804706 | 0.27009668 | 0.95848643 | 44.62737   | 8          |\n",
       "| 130.072200 | 40.463308  | 1.8951732  | 0.23066323 | 92.404291  | 0.79270186 | 0.18821791 | 0.26481386 | 52.10164   | 8          |\n",
       "| 270.502395 | 34.186131  | 1.3370186  | 0.33041472 | 93.966827  | 0.29134681 | 0.63820563 | 0.82755395 | 42.25718   | 8          |\n",
       "|  38.009260 | 88.114134  | 0.1159223  | 0.75591975 | 26.677360  | 0.49086524 | 0.51166162 | 0.51026560 | 49.48517   | 8          |\n",
       "| 105.945047 | 13.406723  | 2.8871286  | 0.18235217 | 24.233537  | 0.91804698 | 0.44660403 | 0.68047562 | 50.16785   | 8          |\n",
       "| 329.459638 | 74.665187  | 4.7043965  | 0.92781913 | 31.755795  | 0.15836933 | 0.69424139 | 0.85872020 | 47.57652   | 8          |\n",
       "| 445.433377 | 20.163957  | 2.8094550  | 0.64871435 | 65.197916  | 0.19703505 | 0.01651411 | 0.27272424 | 85.44612   | 8          |\n",
       "| 205.299918 | 82.925215  | 4.2510232  | 0.06885088 | 40.729035  | 0.50699671 | 0.78870993 | 0.03180102 | 46.33829   | 8          |\n",
       "| 172.020981 | 90.914017  | 0.7537905  | 0.04963815 | 58.502033  | 0.61020586 | 0.38904975 | 0.87583322 | 45.21431   | 8          |\n",
       "| 460.201816 | 96.436939  | 3.2751746  | 0.41358148 | 72.334957  | 0.97636854 | 0.24401142 | 0.08920294 | 77.54047   | 8          |\n",
       "| 100.811706 | 29.419096  | 0.5983277  | 0.81937961 | 73.939981  | 0.54614157 | 0.10760606 | 0.62401049 | 43.65235   | 8          |\n",
       "| 344.588980 | 56.761968  | 1.2317070  | 0.88367730 | 14.666690  | 0.80958359 | 0.91395235 | 0.90586485 | 48.84483   | 8          |\n",
       "| 497.046259 |  6.409169  | 3.4342425  | 0.10687218 | 17.011417  | 0.11113822 | 0.85546841 | 0.64210018 | 48.49530   | 8          |\n",
       "|  33.715980 | 96.878792  | 3.6883370  | 0.01331588 |  9.841476  | 0.06459131 | 0.57659928 | 0.98300466 | 62.21116   | 8          |\n",
       "|  73.873047 | 12.010874  | 4.3941673  | 0.28659854 | 22.609673  | 0.03020353 | 0.04830248 | 0.22913679 | 95.87331   | 8          |\n",
       "| 186.345115 | 85.015145  | 0.9108377  | 0.54842970 | 87.681003  | 0.07166630 | 0.80328097 | 0.11883268 | 43.34818   | 8          |\n",
       "|   7.535025 | 26.249094  | 2.2641867  | 0.94330607 |  1.057592  | 0.83857927 | 0.98857611 | 0.05042143 | 52.57788   | 8          |\n",
       "| 468.942075 |  7.031200  | 1.0194360  | 0.14049176 | 98.277931  | 0.89776848 | 0.96635918 | 0.14922773 | 41.99267   | 8          |\n",
       "| 409.844151 | 45.865906  | 0.1690033  | 0.58936029 |  3.752769  | 0.63345464 | 0.15235143 | 0.18114986 | 58.78656   | 8          |\n",
       "| 158.671688 | 49.715070  | 4.9558383  | 0.52870032 | 85.371960  | 0.70624448 | 0.62319670 | 0.75789638 | 42.77706   | 8          |\n",
       "\n"
      ],
      "text/plain": [
       "   max_links  evidence  sc-bel-prop prop-likelihood n_init_believers prior-mean\n",
       "1  379.030421 33.093892 2.3776148   0.45018803      81.489234        0.25385494\n",
       "2  354.720810 53.770374 1.7306876   0.39097777      51.108860        0.30752116\n",
       "3  150.663956 52.282630 2.5397536   0.35723122      49.568888        0.46417893\n",
       "4  310.303729 37.939215 1.6481274   0.79260720      39.472428        0.41508232\n",
       "5  246.681021 69.824069 2.1237128   0.49174655      79.052799        0.66834203\n",
       "6  284.665309  2.769528 4.5423103   0.62618815      36.565307        0.37020836\n",
       "7   68.156233 17.351079 3.1303719   0.69802720      46.491671        0.35416725\n",
       "8  395.927951 72.373656 4.0555379   0.25140501      54.704373        0.74388255\n",
       "9  265.703127 79.724977 3.9051546   0.71919613      60.588566        0.21410364\n",
       "10 223.475741 60.791212 3.5232455   0.99764996      11.128286        0.93562664\n",
       "11 421.236560 66.295107 0.3364330   0.86289680      69.176981        0.58804706\n",
       "12 130.072200 40.463308 1.8951732   0.23066323      92.404291        0.79270186\n",
       "13 270.502395 34.186131 1.3370186   0.33041472      93.966827        0.29134681\n",
       "14  38.009260 88.114134 0.1159223   0.75591975      26.677360        0.49086524\n",
       "15 105.945047 13.406723 2.8871286   0.18235217      24.233537        0.91804698\n",
       "16 329.459638 74.665187 4.7043965   0.92781913      31.755795        0.15836933\n",
       "17 445.433377 20.163957 2.8094550   0.64871435      65.197916        0.19703505\n",
       "18 205.299918 82.925215 4.2510232   0.06885088      40.729035        0.50699671\n",
       "19 172.020981 90.914017 0.7537905   0.04963815      58.502033        0.61020586\n",
       "20 460.201816 96.436939 3.2751746   0.41358148      72.334957        0.97636854\n",
       "21 100.811706 29.419096 0.5983277   0.81937961      73.939981        0.54614157\n",
       "22 344.588980 56.761968 1.2317070   0.88367730      14.666690        0.80958359\n",
       "23 497.046259  6.409169 3.4342425   0.10687218      17.011417        0.11113822\n",
       "24  33.715980 96.878792 3.6883370   0.01331588       9.841476        0.06459131\n",
       "25  73.873047 12.010874 4.3941673   0.28659854      22.609673        0.03020353\n",
       "26 186.345115 85.015145 0.9108377   0.54842970      87.681003        0.07166630\n",
       "27   7.535025 26.249094 2.2641867   0.94330607       1.057592        0.83857927\n",
       "28 468.942075  7.031200 1.0194360   0.14049176      98.277931        0.89776848\n",
       "29 409.844151 45.865906 0.1690033   0.58936029       3.752769        0.63345464\n",
       "30 158.671688 49.715070 4.9558383   0.52870032      85.371960        0.70624448\n",
       "   prior-sd   expertise_influence output   seed\n",
       "1  0.55607425 0.44670360          42.97597 8   \n",
       "2  0.74643066 0.49186844          45.43132 8   \n",
       "3  0.20238519 0.59242957          45.53525 8   \n",
       "4  0.49821941 0.54886696          46.41394 8   \n",
       "5  0.87004269 0.41265428          43.12122 8   \n",
       "6  0.42145825 0.70496254          46.85059 8   \n",
       "7  0.72813793 0.35974346          45.94112 8   \n",
       "8  0.07464179 0.76934667          45.96783 8   \n",
       "9  0.32198382 0.32189261          47.28248 8   \n",
       "10 0.33789561 0.36937890          53.15028 8   \n",
       "11 0.27009668 0.95848643          44.62737 8   \n",
       "12 0.18821791 0.26481386          52.10164 8   \n",
       "13 0.63820563 0.82755395          42.25718 8   \n",
       "14 0.51166162 0.51026560          49.48517 8   \n",
       "15 0.44660403 0.68047562          50.16785 8   \n",
       "16 0.69424139 0.85872020          47.57652 8   \n",
       "17 0.01651411 0.27272424          85.44612 8   \n",
       "18 0.78870993 0.03180102          46.33829 8   \n",
       "19 0.38904975 0.87583322          45.21431 8   \n",
       "20 0.24401142 0.08920294          77.54047 8   \n",
       "21 0.10760606 0.62401049          43.65235 8   \n",
       "22 0.91395235 0.90586485          48.84483 8   \n",
       "23 0.85546841 0.64210018          48.49530 8   \n",
       "24 0.57659928 0.98300466          62.21116 8   \n",
       "25 0.04830248 0.22913679          95.87331 8   \n",
       "26 0.80328097 0.11883268          43.34818 8   \n",
       "27 0.98857611 0.05042143          52.57788 8   \n",
       "28 0.96635918 0.14922773          41.99267 8   \n",
       "29 0.15235143 0.18114986          58.78656 8   \n",
       "30 0.62319670 0.75789638          42.77706 8   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_shot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8195cd",
   "metadata": {},
   "source": [
    "### One-shot Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce4b3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.type = \"oneshot\"\n",
    "\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "iter = 1\n",
    "for (i in seed.focus) {\n",
    "    \n",
    "    training_set = copy(one_shot_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "    for (r in metarep) {\n",
    "        set.seed(i + r)\n",
    "        run_log_entry()\n",
    "\n",
    "        model_Sub <- randomForest(x = training_set[, -c(\"output\")], y = training_set$output, importance = TRUE, ntree = ntree, mtry = mtry, nperm = nperm)      \n",
    "        model_Sub.path = paste0(outputs.path,models.folder,\"model_\",sample.type,\"_seed_\", i, \"_rep_\", r,\"_size_\",train_ins_oneshot,\".rds\")\n",
    "        saveRDS(model_Sub, model_Sub.path)\n",
    "        \n",
    "        # write errors \n",
    "        obb_err = obb_error_func(model_Sub)     \n",
    "        fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "            ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "        \n",
    "        write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "        write_importance.rf(i,r,iter,model_Sub,sample.type)\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc070",
   "metadata": {},
   "source": [
    "### Adaptive Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa4840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = upload_training_set(model.type,seed.focus,train_ins_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b2e7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>max_links</th><th scope=col>evidence</th><th scope=col>sc-bel-prop</th><th scope=col>prop-likelihood</th><th scope=col>n_init_believers</th><th scope=col>prior-mean</th><th scope=col>prior-sd</th><th scope=col>expertise_influence</th><th scope=col>output</th><th scope=col>seed</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>379.030421</td><td>33.093892 </td><td>2.3776148 </td><td>0.45018803</td><td>81.489234 </td><td>0.25385494</td><td>0.55607425</td><td>0.44670360</td><td>42.97597  </td><td>8         </td></tr>\n",
       "\t<tr><td>354.720810</td><td>53.770374 </td><td>1.7306876 </td><td>0.39097777</td><td>51.108860 </td><td>0.30752116</td><td>0.74643066</td><td>0.49186844</td><td>45.43132  </td><td>8         </td></tr>\n",
       "\t<tr><td>150.663956</td><td>52.282630 </td><td>2.5397536 </td><td>0.35723122</td><td>49.568888 </td><td>0.46417893</td><td>0.20238519</td><td>0.59242957</td><td>45.53525  </td><td>8         </td></tr>\n",
       "\t<tr><td>310.303729</td><td>37.939215 </td><td>1.6481274 </td><td>0.79260720</td><td>39.472428 </td><td>0.41508232</td><td>0.49821941</td><td>0.54886696</td><td>46.41394  </td><td>8         </td></tr>\n",
       "\t<tr><td>246.681021</td><td>69.824069 </td><td>2.1237128 </td><td>0.49174655</td><td>79.052799 </td><td>0.66834203</td><td>0.87004269</td><td>0.41265428</td><td>43.12122  </td><td>8         </td></tr>\n",
       "\t<tr><td>284.665309</td><td> 2.769528 </td><td>4.5423103 </td><td>0.62618815</td><td>36.565307 </td><td>0.37020836</td><td>0.42145825</td><td>0.70496254</td><td>46.85059  </td><td>8         </td></tr>\n",
       "\t<tr><td> 68.156233</td><td>17.351079 </td><td>3.1303719 </td><td>0.69802720</td><td>46.491671 </td><td>0.35416725</td><td>0.72813793</td><td>0.35974346</td><td>45.94112  </td><td>8         </td></tr>\n",
       "\t<tr><td>395.927951</td><td>72.373656 </td><td>4.0555379 </td><td>0.25140501</td><td>54.704373 </td><td>0.74388255</td><td>0.07464179</td><td>0.76934667</td><td>45.96783  </td><td>8         </td></tr>\n",
       "\t<tr><td>265.703127</td><td>79.724977 </td><td>3.9051546 </td><td>0.71919613</td><td>60.588566 </td><td>0.21410364</td><td>0.32198382</td><td>0.32189261</td><td>47.28248  </td><td>8         </td></tr>\n",
       "\t<tr><td>223.475741</td><td>60.791212 </td><td>3.5232455 </td><td>0.99764996</td><td>11.128286 </td><td>0.93562664</td><td>0.33789561</td><td>0.36937890</td><td>53.15028  </td><td>8         </td></tr>\n",
       "\t<tr><td>421.236560</td><td>66.295107 </td><td>0.3364330 </td><td>0.86289680</td><td>69.176981 </td><td>0.58804706</td><td>0.27009668</td><td>0.95848643</td><td>44.62737  </td><td>8         </td></tr>\n",
       "\t<tr><td>130.072200</td><td>40.463308 </td><td>1.8951732 </td><td>0.23066323</td><td>92.404291 </td><td>0.79270186</td><td>0.18821791</td><td>0.26481386</td><td>52.10164  </td><td>8         </td></tr>\n",
       "\t<tr><td>270.502395</td><td>34.186131 </td><td>1.3370186 </td><td>0.33041472</td><td>93.966827 </td><td>0.29134681</td><td>0.63820563</td><td>0.82755395</td><td>42.25718  </td><td>8         </td></tr>\n",
       "\t<tr><td> 38.009260</td><td>88.114134 </td><td>0.1159223 </td><td>0.75591975</td><td>26.677360 </td><td>0.49086524</td><td>0.51166162</td><td>0.51026560</td><td>49.48517  </td><td>8         </td></tr>\n",
       "\t<tr><td>105.945047</td><td>13.406723 </td><td>2.8871286 </td><td>0.18235217</td><td>24.233537 </td><td>0.91804698</td><td>0.44660403</td><td>0.68047562</td><td>50.16785  </td><td>8         </td></tr>\n",
       "\t<tr><td>329.459638</td><td>74.665187 </td><td>4.7043965 </td><td>0.92781913</td><td>31.755795 </td><td>0.15836933</td><td>0.69424139</td><td>0.85872020</td><td>47.57652  </td><td>8         </td></tr>\n",
       "\t<tr><td>445.433377</td><td>20.163957 </td><td>2.8094550 </td><td>0.64871435</td><td>65.197916 </td><td>0.19703505</td><td>0.01651411</td><td>0.27272424</td><td>85.44612  </td><td>8         </td></tr>\n",
       "\t<tr><td>205.299918</td><td>82.925215 </td><td>4.2510232 </td><td>0.06885088</td><td>40.729035 </td><td>0.50699671</td><td>0.78870993</td><td>0.03180102</td><td>46.33829  </td><td>8         </td></tr>\n",
       "\t<tr><td>172.020981</td><td>90.914017 </td><td>0.7537905 </td><td>0.04963815</td><td>58.502033 </td><td>0.61020586</td><td>0.38904975</td><td>0.87583322</td><td>45.21431  </td><td>8         </td></tr>\n",
       "\t<tr><td>460.201816</td><td>96.436939 </td><td>3.2751746 </td><td>0.41358148</td><td>72.334957 </td><td>0.97636854</td><td>0.24401142</td><td>0.08920294</td><td>77.54047  </td><td>8         </td></tr>\n",
       "\t<tr><td>100.811706</td><td>29.419096 </td><td>0.5983277 </td><td>0.81937961</td><td>73.939981 </td><td>0.54614157</td><td>0.10760606</td><td>0.62401049</td><td>43.65235  </td><td>8         </td></tr>\n",
       "\t<tr><td>344.588980</td><td>56.761968 </td><td>1.2317070 </td><td>0.88367730</td><td>14.666690 </td><td>0.80958359</td><td>0.91395235</td><td>0.90586485</td><td>48.84483  </td><td>8         </td></tr>\n",
       "\t<tr><td>497.046259</td><td> 6.409169 </td><td>3.4342425 </td><td>0.10687218</td><td>17.011417 </td><td>0.11113822</td><td>0.85546841</td><td>0.64210018</td><td>48.49530  </td><td>8         </td></tr>\n",
       "\t<tr><td> 33.715980</td><td>96.878792 </td><td>3.6883370 </td><td>0.01331588</td><td> 9.841476 </td><td>0.06459131</td><td>0.57659928</td><td>0.98300466</td><td>62.21116  </td><td>8         </td></tr>\n",
       "\t<tr><td> 73.873047</td><td>12.010874 </td><td>4.3941673 </td><td>0.28659854</td><td>22.609673 </td><td>0.03020353</td><td>0.04830248</td><td>0.22913679</td><td>95.87331  </td><td>8         </td></tr>\n",
       "\t<tr><td>186.345115</td><td>85.015145 </td><td>0.9108377 </td><td>0.54842970</td><td>87.681003 </td><td>0.07166630</td><td>0.80328097</td><td>0.11883268</td><td>43.34818  </td><td>8         </td></tr>\n",
       "\t<tr><td>  7.535025</td><td>26.249094 </td><td>2.2641867 </td><td>0.94330607</td><td> 1.057592 </td><td>0.83857927</td><td>0.98857611</td><td>0.05042143</td><td>52.57788  </td><td>8         </td></tr>\n",
       "\t<tr><td>468.942075</td><td> 7.031200 </td><td>1.0194360 </td><td>0.14049176</td><td>98.277931 </td><td>0.89776848</td><td>0.96635918</td><td>0.14922773</td><td>41.99267  </td><td>8         </td></tr>\n",
       "\t<tr><td>409.844151</td><td>45.865906 </td><td>0.1690033 </td><td>0.58936029</td><td> 3.752769 </td><td>0.63345464</td><td>0.15235143</td><td>0.18114986</td><td>58.78656  </td><td>8         </td></tr>\n",
       "\t<tr><td>158.671688</td><td>49.715070 </td><td>4.9558383 </td><td>0.52870032</td><td>85.371960 </td><td>0.70624448</td><td>0.62319670</td><td>0.75789638</td><td>42.77706  </td><td>8         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " max\\_links & evidence & sc-bel-prop & prop-likelihood & n\\_init\\_believers & prior-mean & prior-sd & expertise\\_influence & output & seed\\\\\n",
       "\\hline\n",
       "\t 379.030421 & 33.093892  & 2.3776148  & 0.45018803 & 81.489234  & 0.25385494 & 0.55607425 & 0.44670360 & 42.97597   & 8         \\\\\n",
       "\t 354.720810 & 53.770374  & 1.7306876  & 0.39097777 & 51.108860  & 0.30752116 & 0.74643066 & 0.49186844 & 45.43132   & 8         \\\\\n",
       "\t 150.663956 & 52.282630  & 2.5397536  & 0.35723122 & 49.568888  & 0.46417893 & 0.20238519 & 0.59242957 & 45.53525   & 8         \\\\\n",
       "\t 310.303729 & 37.939215  & 1.6481274  & 0.79260720 & 39.472428  & 0.41508232 & 0.49821941 & 0.54886696 & 46.41394   & 8         \\\\\n",
       "\t 246.681021 & 69.824069  & 2.1237128  & 0.49174655 & 79.052799  & 0.66834203 & 0.87004269 & 0.41265428 & 43.12122   & 8         \\\\\n",
       "\t 284.665309 &  2.769528  & 4.5423103  & 0.62618815 & 36.565307  & 0.37020836 & 0.42145825 & 0.70496254 & 46.85059   & 8         \\\\\n",
       "\t  68.156233 & 17.351079  & 3.1303719  & 0.69802720 & 46.491671  & 0.35416725 & 0.72813793 & 0.35974346 & 45.94112   & 8         \\\\\n",
       "\t 395.927951 & 72.373656  & 4.0555379  & 0.25140501 & 54.704373  & 0.74388255 & 0.07464179 & 0.76934667 & 45.96783   & 8         \\\\\n",
       "\t 265.703127 & 79.724977  & 3.9051546  & 0.71919613 & 60.588566  & 0.21410364 & 0.32198382 & 0.32189261 & 47.28248   & 8         \\\\\n",
       "\t 223.475741 & 60.791212  & 3.5232455  & 0.99764996 & 11.128286  & 0.93562664 & 0.33789561 & 0.36937890 & 53.15028   & 8         \\\\\n",
       "\t 421.236560 & 66.295107  & 0.3364330  & 0.86289680 & 69.176981  & 0.58804706 & 0.27009668 & 0.95848643 & 44.62737   & 8         \\\\\n",
       "\t 130.072200 & 40.463308  & 1.8951732  & 0.23066323 & 92.404291  & 0.79270186 & 0.18821791 & 0.26481386 & 52.10164   & 8         \\\\\n",
       "\t 270.502395 & 34.186131  & 1.3370186  & 0.33041472 & 93.966827  & 0.29134681 & 0.63820563 & 0.82755395 & 42.25718   & 8         \\\\\n",
       "\t  38.009260 & 88.114134  & 0.1159223  & 0.75591975 & 26.677360  & 0.49086524 & 0.51166162 & 0.51026560 & 49.48517   & 8         \\\\\n",
       "\t 105.945047 & 13.406723  & 2.8871286  & 0.18235217 & 24.233537  & 0.91804698 & 0.44660403 & 0.68047562 & 50.16785   & 8         \\\\\n",
       "\t 329.459638 & 74.665187  & 4.7043965  & 0.92781913 & 31.755795  & 0.15836933 & 0.69424139 & 0.85872020 & 47.57652   & 8         \\\\\n",
       "\t 445.433377 & 20.163957  & 2.8094550  & 0.64871435 & 65.197916  & 0.19703505 & 0.01651411 & 0.27272424 & 85.44612   & 8         \\\\\n",
       "\t 205.299918 & 82.925215  & 4.2510232  & 0.06885088 & 40.729035  & 0.50699671 & 0.78870993 & 0.03180102 & 46.33829   & 8         \\\\\n",
       "\t 172.020981 & 90.914017  & 0.7537905  & 0.04963815 & 58.502033  & 0.61020586 & 0.38904975 & 0.87583322 & 45.21431   & 8         \\\\\n",
       "\t 460.201816 & 96.436939  & 3.2751746  & 0.41358148 & 72.334957  & 0.97636854 & 0.24401142 & 0.08920294 & 77.54047   & 8         \\\\\n",
       "\t 100.811706 & 29.419096  & 0.5983277  & 0.81937961 & 73.939981  & 0.54614157 & 0.10760606 & 0.62401049 & 43.65235   & 8         \\\\\n",
       "\t 344.588980 & 56.761968  & 1.2317070  & 0.88367730 & 14.666690  & 0.80958359 & 0.91395235 & 0.90586485 & 48.84483   & 8         \\\\\n",
       "\t 497.046259 &  6.409169  & 3.4342425  & 0.10687218 & 17.011417  & 0.11113822 & 0.85546841 & 0.64210018 & 48.49530   & 8         \\\\\n",
       "\t  33.715980 & 96.878792  & 3.6883370  & 0.01331588 &  9.841476  & 0.06459131 & 0.57659928 & 0.98300466 & 62.21116   & 8         \\\\\n",
       "\t  73.873047 & 12.010874  & 4.3941673  & 0.28659854 & 22.609673  & 0.03020353 & 0.04830248 & 0.22913679 & 95.87331   & 8         \\\\\n",
       "\t 186.345115 & 85.015145  & 0.9108377  & 0.54842970 & 87.681003  & 0.07166630 & 0.80328097 & 0.11883268 & 43.34818   & 8         \\\\\n",
       "\t   7.535025 & 26.249094  & 2.2641867  & 0.94330607 &  1.057592  & 0.83857927 & 0.98857611 & 0.05042143 & 52.57788   & 8         \\\\\n",
       "\t 468.942075 &  7.031200  & 1.0194360  & 0.14049176 & 98.277931  & 0.89776848 & 0.96635918 & 0.14922773 & 41.99267   & 8         \\\\\n",
       "\t 409.844151 & 45.865906  & 0.1690033  & 0.58936029 &  3.752769  & 0.63345464 & 0.15235143 & 0.18114986 & 58.78656   & 8         \\\\\n",
       "\t 158.671688 & 49.715070  & 4.9558383  & 0.52870032 & 85.371960  & 0.70624448 & 0.62319670 & 0.75789638 & 42.77706   & 8         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| max_links | evidence | sc-bel-prop | prop-likelihood | n_init_believers | prior-mean | prior-sd | expertise_influence | output | seed |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 379.030421 | 33.093892  | 2.3776148  | 0.45018803 | 81.489234  | 0.25385494 | 0.55607425 | 0.44670360 | 42.97597   | 8          |\n",
       "| 354.720810 | 53.770374  | 1.7306876  | 0.39097777 | 51.108860  | 0.30752116 | 0.74643066 | 0.49186844 | 45.43132   | 8          |\n",
       "| 150.663956 | 52.282630  | 2.5397536  | 0.35723122 | 49.568888  | 0.46417893 | 0.20238519 | 0.59242957 | 45.53525   | 8          |\n",
       "| 310.303729 | 37.939215  | 1.6481274  | 0.79260720 | 39.472428  | 0.41508232 | 0.49821941 | 0.54886696 | 46.41394   | 8          |\n",
       "| 246.681021 | 69.824069  | 2.1237128  | 0.49174655 | 79.052799  | 0.66834203 | 0.87004269 | 0.41265428 | 43.12122   | 8          |\n",
       "| 284.665309 |  2.769528  | 4.5423103  | 0.62618815 | 36.565307  | 0.37020836 | 0.42145825 | 0.70496254 | 46.85059   | 8          |\n",
       "|  68.156233 | 17.351079  | 3.1303719  | 0.69802720 | 46.491671  | 0.35416725 | 0.72813793 | 0.35974346 | 45.94112   | 8          |\n",
       "| 395.927951 | 72.373656  | 4.0555379  | 0.25140501 | 54.704373  | 0.74388255 | 0.07464179 | 0.76934667 | 45.96783   | 8          |\n",
       "| 265.703127 | 79.724977  | 3.9051546  | 0.71919613 | 60.588566  | 0.21410364 | 0.32198382 | 0.32189261 | 47.28248   | 8          |\n",
       "| 223.475741 | 60.791212  | 3.5232455  | 0.99764996 | 11.128286  | 0.93562664 | 0.33789561 | 0.36937890 | 53.15028   | 8          |\n",
       "| 421.236560 | 66.295107  | 0.3364330  | 0.86289680 | 69.176981  | 0.58804706 | 0.27009668 | 0.95848643 | 44.62737   | 8          |\n",
       "| 130.072200 | 40.463308  | 1.8951732  | 0.23066323 | 92.404291  | 0.79270186 | 0.18821791 | 0.26481386 | 52.10164   | 8          |\n",
       "| 270.502395 | 34.186131  | 1.3370186  | 0.33041472 | 93.966827  | 0.29134681 | 0.63820563 | 0.82755395 | 42.25718   | 8          |\n",
       "|  38.009260 | 88.114134  | 0.1159223  | 0.75591975 | 26.677360  | 0.49086524 | 0.51166162 | 0.51026560 | 49.48517   | 8          |\n",
       "| 105.945047 | 13.406723  | 2.8871286  | 0.18235217 | 24.233537  | 0.91804698 | 0.44660403 | 0.68047562 | 50.16785   | 8          |\n",
       "| 329.459638 | 74.665187  | 4.7043965  | 0.92781913 | 31.755795  | 0.15836933 | 0.69424139 | 0.85872020 | 47.57652   | 8          |\n",
       "| 445.433377 | 20.163957  | 2.8094550  | 0.64871435 | 65.197916  | 0.19703505 | 0.01651411 | 0.27272424 | 85.44612   | 8          |\n",
       "| 205.299918 | 82.925215  | 4.2510232  | 0.06885088 | 40.729035  | 0.50699671 | 0.78870993 | 0.03180102 | 46.33829   | 8          |\n",
       "| 172.020981 | 90.914017  | 0.7537905  | 0.04963815 | 58.502033  | 0.61020586 | 0.38904975 | 0.87583322 | 45.21431   | 8          |\n",
       "| 460.201816 | 96.436939  | 3.2751746  | 0.41358148 | 72.334957  | 0.97636854 | 0.24401142 | 0.08920294 | 77.54047   | 8          |\n",
       "| 100.811706 | 29.419096  | 0.5983277  | 0.81937961 | 73.939981  | 0.54614157 | 0.10760606 | 0.62401049 | 43.65235   | 8          |\n",
       "| 344.588980 | 56.761968  | 1.2317070  | 0.88367730 | 14.666690  | 0.80958359 | 0.91395235 | 0.90586485 | 48.84483   | 8          |\n",
       "| 497.046259 |  6.409169  | 3.4342425  | 0.10687218 | 17.011417  | 0.11113822 | 0.85546841 | 0.64210018 | 48.49530   | 8          |\n",
       "|  33.715980 | 96.878792  | 3.6883370  | 0.01331588 |  9.841476  | 0.06459131 | 0.57659928 | 0.98300466 | 62.21116   | 8          |\n",
       "|  73.873047 | 12.010874  | 4.3941673  | 0.28659854 | 22.609673  | 0.03020353 | 0.04830248 | 0.22913679 | 95.87331   | 8          |\n",
       "| 186.345115 | 85.015145  | 0.9108377  | 0.54842970 | 87.681003  | 0.07166630 | 0.80328097 | 0.11883268 | 43.34818   | 8          |\n",
       "|   7.535025 | 26.249094  | 2.2641867  | 0.94330607 |  1.057592  | 0.83857927 | 0.98857611 | 0.05042143 | 52.57788   | 8          |\n",
       "| 468.942075 |  7.031200  | 1.0194360  | 0.14049176 | 98.277931  | 0.89776848 | 0.96635918 | 0.14922773 | 41.99267   | 8          |\n",
       "| 409.844151 | 45.865906  | 0.1690033  | 0.58936029 |  3.752769  | 0.63345464 | 0.15235143 | 0.18114986 | 58.78656   | 8          |\n",
       "| 158.671688 | 49.715070  | 4.9558383  | 0.52870032 | 85.371960  | 0.70624448 | 0.62319670 | 0.75789638 | 42.77706   | 8          |\n",
       "\n"
      ],
      "text/plain": [
       "   max_links  evidence  sc-bel-prop prop-likelihood n_init_believers prior-mean\n",
       "1  379.030421 33.093892 2.3776148   0.45018803      81.489234        0.25385494\n",
       "2  354.720810 53.770374 1.7306876   0.39097777      51.108860        0.30752116\n",
       "3  150.663956 52.282630 2.5397536   0.35723122      49.568888        0.46417893\n",
       "4  310.303729 37.939215 1.6481274   0.79260720      39.472428        0.41508232\n",
       "5  246.681021 69.824069 2.1237128   0.49174655      79.052799        0.66834203\n",
       "6  284.665309  2.769528 4.5423103   0.62618815      36.565307        0.37020836\n",
       "7   68.156233 17.351079 3.1303719   0.69802720      46.491671        0.35416725\n",
       "8  395.927951 72.373656 4.0555379   0.25140501      54.704373        0.74388255\n",
       "9  265.703127 79.724977 3.9051546   0.71919613      60.588566        0.21410364\n",
       "10 223.475741 60.791212 3.5232455   0.99764996      11.128286        0.93562664\n",
       "11 421.236560 66.295107 0.3364330   0.86289680      69.176981        0.58804706\n",
       "12 130.072200 40.463308 1.8951732   0.23066323      92.404291        0.79270186\n",
       "13 270.502395 34.186131 1.3370186   0.33041472      93.966827        0.29134681\n",
       "14  38.009260 88.114134 0.1159223   0.75591975      26.677360        0.49086524\n",
       "15 105.945047 13.406723 2.8871286   0.18235217      24.233537        0.91804698\n",
       "16 329.459638 74.665187 4.7043965   0.92781913      31.755795        0.15836933\n",
       "17 445.433377 20.163957 2.8094550   0.64871435      65.197916        0.19703505\n",
       "18 205.299918 82.925215 4.2510232   0.06885088      40.729035        0.50699671\n",
       "19 172.020981 90.914017 0.7537905   0.04963815      58.502033        0.61020586\n",
       "20 460.201816 96.436939 3.2751746   0.41358148      72.334957        0.97636854\n",
       "21 100.811706 29.419096 0.5983277   0.81937961      73.939981        0.54614157\n",
       "22 344.588980 56.761968 1.2317070   0.88367730      14.666690        0.80958359\n",
       "23 497.046259  6.409169 3.4342425   0.10687218      17.011417        0.11113822\n",
       "24  33.715980 96.878792 3.6883370   0.01331588       9.841476        0.06459131\n",
       "25  73.873047 12.010874 4.3941673   0.28659854      22.609673        0.03020353\n",
       "26 186.345115 85.015145 0.9108377   0.54842970      87.681003        0.07166630\n",
       "27   7.535025 26.249094 2.2641867   0.94330607       1.057592        0.83857927\n",
       "28 468.942075  7.031200 1.0194360   0.14049176      98.277931        0.89776848\n",
       "29 409.844151 45.865906 0.1690033   0.58936029       3.752769        0.63345464\n",
       "30 158.671688 49.715070 4.9558383   0.52870032      85.371960        0.70624448\n",
       "   prior-sd   expertise_influence output   seed\n",
       "1  0.55607425 0.44670360          42.97597 8   \n",
       "2  0.74643066 0.49186844          45.43132 8   \n",
       "3  0.20238519 0.59242957          45.53525 8   \n",
       "4  0.49821941 0.54886696          46.41394 8   \n",
       "5  0.87004269 0.41265428          43.12122 8   \n",
       "6  0.42145825 0.70496254          46.85059 8   \n",
       "7  0.72813793 0.35974346          45.94112 8   \n",
       "8  0.07464179 0.76934667          45.96783 8   \n",
       "9  0.32198382 0.32189261          47.28248 8   \n",
       "10 0.33789561 0.36937890          53.15028 8   \n",
       "11 0.27009668 0.95848643          44.62737 8   \n",
       "12 0.18821791 0.26481386          52.10164 8   \n",
       "13 0.63820563 0.82755395          42.25718 8   \n",
       "14 0.51166162 0.51026560          49.48517 8   \n",
       "15 0.44660403 0.68047562          50.16785 8   \n",
       "16 0.69424139 0.85872020          47.57652 8   \n",
       "17 0.01651411 0.27272424          85.44612 8   \n",
       "18 0.78870993 0.03180102          46.33829 8   \n",
       "19 0.38904975 0.87583322          45.21431 8   \n",
       "20 0.24401142 0.08920294          77.54047 8   \n",
       "21 0.10760606 0.62401049          43.65235 8   \n",
       "22 0.91395235 0.90586485          48.84483 8   \n",
       "23 0.85546841 0.64210018          48.49530 8   \n",
       "24 0.57659928 0.98300466          62.21116 8   \n",
       "25 0.04830248 0.22913679          95.87331 8   \n",
       "26 0.80328097 0.11883268          43.34818 8   \n",
       "27 0.98857611 0.05042143          52.57788 8   \n",
       "28 0.96635918 0.14922773          41.99267 8   \n",
       "29 0.15235143 0.18114986          58.78656 8   \n",
       "30 0.62319670 0.75789638          42.77706 8   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptive_initial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca5fe9",
   "metadata": {},
   "source": [
    "### Random Sampling Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431dc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"seed : 8  Random Sampling section start time : 2022-01-07 15:57:24\"\n",
      "[1] \"seed : 8   rep : 1  Random Sampling section start time : 2022-01-07 15:57:24\"\n",
      "[1] 1\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 15:57:24\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:00:40\"\n",
      "[1] 2\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:00:40\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:02:47\"\n",
      "[1] 3\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:02:47\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:05:51\"\n",
      "[1] 4\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:05:51\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:08:20\"\n",
      "[1] 5\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:08:20\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:11:02\"\n",
      "[1] 6\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:11:02\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:13:51\"\n",
      "[1] 7\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:13:51\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:18:57\"\n",
      "[1] 8\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:18:57\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:21:25\"\n",
      "[1] 9\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:21:25\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:28:04\"\n",
      "[1] 10\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:28:04\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:33:25\"\n",
      "[1] 11\n",
      "[1] \"seed : 8   rep : 1  Random Sampling section end time : 2022-01-07 16:33:25\"\n",
      "[1] \"seed : 8  Random Sampling section end time : 2022-01-07 16:33:25\"\n"
     ]
    }
   ],
   "source": [
    "sample.type = \"Rd\"\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "for (i in seed.focus) {\n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section start time : \", Sys.time()))\n",
    "    \n",
    "    for (r in metarep) { #replications\n",
    "        set.seed(i + r) # set seed to control randomness\n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section start time : \", Sys.time()))\n",
    "        \n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])       \n",
    "        train_candidates_table = data.table()\n",
    "            \n",
    "        iter = 1\n",
    "        while(iter <= iteration_budget){\n",
    "            print(iter)\n",
    "            run_log_entry()\n",
    "            \n",
    "            trainx = training_set_Ad[, .SD, .SDcols = feature_names]\n",
    "            trainy = training_set_Ad$output\n",
    "            \n",
    "            # Train the model\n",
    "            model_Sub <- randomForest(x = trainx, y = trainy, importance = TRUE,ntree = ntree, mtry = mtry, nperm = nperm)\n",
    "            model_Sub.name = paste0(\"model_\",sample.type,\"_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "            assign(model_Sub.name, model_Sub)\n",
    "            model_Sub.path = paste0(outputs.path,models.folder, paste0(model_Sub.name,\"_size_\",train_ins_Ad, \".rds\"))  # to save the model\n",
    "                saveRDS(model_Sub, model_Sub.path)\n",
    "            \n",
    "            # write errors \n",
    "            obb_err = obb_error_func(model_Sub)     \n",
    "            fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "                   ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "            \n",
    "            write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "            write_importance.rf(i,r,iter,model_Sub,sample.type)#last one=sample_type\n",
    "\n",
    "                   \n",
    "            if (iter != iteration_budget) {\n",
    "                ## sample selection from unlabeled data select candidates\n",
    "                 \n",
    "                unlabeled_set <- refresh_sample_pool(i + r + iter)\n",
    "                train_candidates = random_sample_selection(selected_ins, unlabeled_set)\n",
    "                                \n",
    "                # run ABM to find outputs of train candidates\n",
    "                print(paste0(\"ABM train_candidate run start time : \", Sys.time()))\n",
    "                train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "                print(paste0(\"ABM train_candidate run end time : \", Sys.time()))\n",
    "                \n",
    "                \n",
    "                fwrite(data.table(train_candidates, \"iter\" = iter, \"seed\" = i, \"rep\" = r)\n",
    "                       ,paste0(outputs.path,sample.folder,model.type,\"_train_candidates_table_\",sample.type,\".csv\"),append = TRUE )               \n",
    " \n",
    "                # Add new data to train data\n",
    "                training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")],use.names = TRUE)\n",
    "            }\n",
    "            iter = iter + 1\n",
    "        }  \n",
    "        \n",
    "        fwrite(data.table(training_set_Ad, \"seed\" = i,\"rep\" = r),paste0(outputs.path,sample.folder,model.type,\"_FinalTrainData_\",sample.type,\".csv\") ,append = TRUE)\n",
    "    \n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section end time : \", Sys.time()))        \n",
    "    }  \n",
    "         \n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section end time : \", Sys.time()))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970a28b",
   "metadata": {},
   "source": [
    "### Adaptive Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d1f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = upload_training_set(model.type,seed.focus,train_ins_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de55b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"seed : 8  Adaptive Sampling with coefvar  section start time : 2022-01-07 16:44:56\"\n",
      "[1] \"seed : 8   rep : 1  Adaptive Sampling section start time : 2022-01-07 16:44:56\"\n",
      "[1] 1\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:44:56\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:47:52\"\n",
      "[1] 2\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:47:52\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:50:47\"\n",
      "[1] 3\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:50:47\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 16:56:41\"\n",
      "[1] 4\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 16:56:41\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:05:40\"\n",
      "[1] 5\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:05:40\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:12:05\"\n",
      "[1] 6\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:12:05\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:14:50\"\n",
      "[1] 7\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:14:50\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:17:38\"\n",
      "[1] 8\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:17:38\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:20:56\"\n",
      "[1] 9\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:20:57\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:23:43\"\n",
      "[1] 10\n",
      "[1] \"ABM train_candidate run start time : 2022-01-07 17:23:43\"\n",
      "[1] \"ABM train_candidate run end time : 2022-01-07 17:26:59\"\n",
      "[1] 11\n",
      "[1] \"seed : 8   rep : 1  Adaptive Sampling section end time : 2022-01-07 17:26:59\"\n",
      "[1] \"seed : 8  Adaptive Sampling section end time : 2022-01-07 17:26:59\"\n"
     ]
    }
   ],
   "source": [
    "sample.type = paste0(\"Ad_\",selection_metric)\n",
    "sample.folder = paste0(sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,sample.folder), showWarnings = FALSE)\n",
    "\n",
    "models.folder = paste0(\"models_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,models.folder), showWarnings = FALSE)\n",
    "\n",
    "PL.folder = paste0(\"PL_\",sample.type,\"/\")\n",
    "dir.create(file.path(folder.path, output.folder,PL.folder), showWarnings = FALSE)\n",
    "\n",
    "for(i in seed.focus){\n",
    "\n",
    "    print(paste0(\"seed : \",i,\"  Adaptive Sampling with \",selection_metric,\"  section start time : \",Sys.time()))\n",
    "    \n",
    "    for (r in metarep){ #replications\n",
    "        set.seed(i + r)\n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Adaptive Sampling section start time : \", Sys.time()))\n",
    "            \n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "        train_candidates_table = data.table()\n",
    "\n",
    "        iter = 1\n",
    "        while(iter <= iteration_budget){   \n",
    "            print(iter)\n",
    "            run_log_entry() \n",
    "        \n",
    "            trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "            trainy = training_set_Ad$output\n",
    "        \n",
    "            # Train the model\n",
    "            model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry,nperm = nperm)\n",
    "            model_Sub.name = paste0(\"model_\",sample.type,\"_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "            model_Sub.path = paste0(outputs.path,models.folder, paste0(model_Sub.name,\"_size_\",train_ins_Ad, \".rds\"))  # to save the model\n",
    "            saveRDS(model_Sub, model_Sub.path)\n",
    "        \n",
    "            # write errors \n",
    "            obb_err = obb_error_func(model_Sub)     \n",
    "            fwrite(data.table(iter,obb_error = obb_err,seed = i,rep = r)\n",
    "                   ,paste0(outputs.path,sample.folder,model.type,\"_\",\"obb_error_\",sample.type,\".csv\") ,append = TRUE)\n",
    "        \n",
    "            write_test_accuracy(i,r,iter,model_Sub,test_set, error_type)\n",
    "            write_importance.rf(i,r,iter,model_Sub,sample.type)#last one=sample_type\n",
    "        \n",
    "            if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.    \n",
    "                ## sample selection from unlabeled data select candidates\n",
    "                unlabeled_set <- refresh_sample_pool(i + r + iter)\n",
    "            \n",
    "                train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub,selection_metric)\n",
    "            \n",
    "                # run ABM to find outputs of train candidates\n",
    "                print(paste0(\"ABM train_candidate run start time : \",Sys.time()))\n",
    "                train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "                print(paste0(\"ABM train_candidate run end time : \",Sys.time()))\n",
    "            \n",
    "                fwrite(data.table(train_candidates, \"iter\" = iter, \"seed\" = i, \"rep\" = r)\n",
    "                       ,paste0(outputs.path,sample.folder,model.type,\"_train_candidates_table_\",sample.type,\".csv\"),append = TRUE )      \n",
    "\n",
    "                # add labeled candidates to the train data\n",
    "                training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")],use.names = TRUE)\n",
    "            }  \n",
    "        \n",
    "            iter = iter + 1\n",
    "        }\n",
    "        fwrite(data.table(training_set_Ad, \"seed\" = i,\"rep\" = r),paste0(outputs.path,sample.folder,model.type,\"_FinalTrainData_\",sample.type,\".csv\") ,append = TRUE)\n",
    "    \n",
    "        print(paste0(\"seed : \",i,\"   rep : \", r,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "    }\n",
    "        \n",
    "    print(paste0(\"seed : \",i,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21251c",
   "metadata": {},
   "source": [
    "## Quit NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f744b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLQuit(nl.obj = nl.model)\n",
    "#NLQuit(all=TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
